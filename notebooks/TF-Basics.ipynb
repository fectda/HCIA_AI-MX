{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programación básica en TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensores: Constantes y Variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensores constantes\n",
    "\n",
    "* tf.constant()\n",
    "* tf.zeros(),tf.ones() \n",
    "* tf.fill()\n",
    "* tf.random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function constant in module tensorflow.python.framework.constant_op:\n",
      "\n",
      "constant(value, dtype=None, shape=None, name='Const')\n",
      "    Creates a constant tensor from a tensor-like object.\n",
      "    \n",
      "    Note: All eager `tf.Tensor` values are immutable (in contrast to\n",
      "    `tf.Variable`). There is nothing especially _constant_ about the value\n",
      "    returned from `tf.constant`. This function it is not fundamentally different\n",
      "    from `tf.convert_to_tensor`. The name `tf.constant` comes from the symbolic\n",
      "    APIs (like `tf.data` or keras functional models) where the `value` is embeded\n",
      "    in a `Const` node in the `tf.Graph`. `tf.constant` is useful for asserting\n",
      "    that the value can be embedded that way.\n",
      "    \n",
      "    If the argument `dtype` is not specified, then the type is inferred from\n",
      "    the type of `value`.\n",
      "    \n",
      "    >>> # Constant 1-D Tensor from a python list.\n",
      "    >>> tf.constant([1, 2, 3, 4, 5, 6])\n",
      "    <tf.Tensor: shape=(6,), dtype=int32,\n",
      "        numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n",
      "    >>> # Or a numpy array\n",
      "    >>> a = np.array([[1, 2, 3], [4, 5, 6]])\n",
      "    >>> tf.constant(a)\n",
      "    <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
      "      array([[1, 2, 3],\n",
      "             [4, 5, 6]])>\n",
      "    \n",
      "    If `dtype` is specified the resulting tensor values are cast to the requested\n",
      "    `dtype`.\n",
      "    \n",
      "    >>> tf.constant([1, 2, 3, 4, 5, 6], dtype=tf.float64)\n",
      "    <tf.Tensor: shape=(6,), dtype=float64,\n",
      "        numpy=array([1., 2., 3., 4., 5., 6.])>\n",
      "    \n",
      "    If `shape` is set, the `value` is reshaped to match. Scalars are expanded to\n",
      "    fill the `shape`:\n",
      "    \n",
      "    >>> tf.constant(0, shape=(2, 3))\n",
      "      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      "      array([[0, 0, 0],\n",
      "             [0, 0, 0]], dtype=int32)>\n",
      "    >>> tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      "    <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      "      array([[1, 2, 3],\n",
      "             [4, 5, 6]], dtype=int32)>\n",
      "    \n",
      "    `tf.constant` has no effect if an eager Tensor is passed as the `value`, it\n",
      "    even transmits gradients:\n",
      "    \n",
      "    >>> v = tf.Variable([0.0])\n",
      "    >>> with tf.GradientTape() as g:\n",
      "    ...     loss = tf.constant(v + v)\n",
      "    >>> g.gradient(loss, v).numpy()\n",
      "    array([2.], dtype=float32)\n",
      "    \n",
      "    But, since `tf.constant` embeds the value in the `tf.Graph` this fails for\n",
      "    symbolic tensors:\n",
      "    \n",
      "    >>> i = tf.keras.layers.Input(shape=[None, None])\n",
      "    >>> t = tf.constant(i)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    NotImplementedError: ...\n",
      "    \n",
      "    `tf.constant` will _always_ create CPU (host) tensors. In order to create\n",
      "    tensors on other devices, use `tf.identity`. (If the `value` is an eager\n",
      "    Tensor, however, the tensor will be returned unmodified as mentioned above.)\n",
      "    \n",
      "    Related Ops:\n",
      "    \n",
      "    * `tf.convert_to_tensor` is similar but:\n",
      "      * It has no `shape` argument.\n",
      "      * Symbolic tensors are allowed to pass through.\n",
      "    \n",
      "        >>> i = tf.keras.layers.Input(shape=[None, None])\n",
      "        >>> t = tf.convert_to_tensor(i)\n",
      "    \n",
      "    * `tf.fill`: differs in a few ways:\n",
      "      *   `tf.constant` supports arbitrary constants, not just uniform scalar\n",
      "          Tensors like `tf.fill`.\n",
      "      *   `tf.fill` creates an Op in the graph that is expanded at runtime, so it\n",
      "          can efficiently represent large tensors.\n",
      "      *   Since `tf.fill` does not embed the value, it can produce dynamically\n",
      "          sized outputs.\n",
      "    \n",
      "    Args:\n",
      "      value: A constant value (or list) of output type `dtype`.\n",
      "      dtype: The type of the elements of the resulting tensor.\n",
      "      shape: Optional dimensions of resulting tensor.\n",
      "      name: Optional name for the tensor.\n",
      "    \n",
      "    Returns:\n",
      "      A Constant Tensor.\n",
      "    \n",
      "    Raises:\n",
      "      TypeError: if shape is incorrectly specified or unsupported.\n",
      "      ValueError: if called on a symbolic tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_a = tf.constant([[1,2,3,4]],shape=[2,2],dtype=tf.float32) # Una matriz de 2x2, con valores del 1 al 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 2.],\n",
       "       [3., 4.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor del tensor CONSTANTE [[1. 2.]\n",
      " [3. 4.]]\n",
      "Tipo de elementos <dtype: 'float32'>\n",
      "Shape del tensor (2, 2)\n"
     ]
    }
   ],
   "source": [
    "#Atributos comunes\n",
    "\n",
    "print('Valor del tensor CONSTANTE', const_a.numpy())\n",
    "print('Tipo de elementos', const_a.dtype)\n",
    "print('Shape del tensor',const_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function zeros in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "zeros(shape, dtype=tf.float32, name=None)\n",
      "    Creates a tensor with all elements set to zero.\n",
      "    \n",
      "    See also `tf.zeros_like`, `tf.ones`, `tf.fill`, `tf.eye`.\n",
      "    \n",
      "    This operation returns a tensor of type `dtype` with shape `shape` and\n",
      "    all elements set to zero.\n",
      "    \n",
      "    >>> tf.zeros([3, 4], tf.int32)\n",
      "    <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
      "    array([[0, 0, 0, 0],\n",
      "           [0, 0, 0, 0],\n",
      "           [0, 0, 0, 0]], dtype=int32)>\n",
      "    \n",
      "    Args:\n",
      "      shape: A `list` of integers, a `tuple` of integers, or\n",
      "        a 1-D `Tensor` of type `int32`.\n",
      "      dtype: The DType of an element in the resulting `Tensor`.\n",
      "      name: Optional string. A name for the operation.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` with all elements set to zero.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_b = tf.zeros([2,3]) #Matriz de 2x2, con todos los valores son 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_b = tf.ones([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fill in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "fill(dims, value, name=None)\n",
      "    Creates a tensor filled with a scalar value.\n",
      "    \n",
      "    See also `tf.ones`, `tf.zeros`, `tf.one_hot`, `tf.eye`.\n",
      "    \n",
      "    This operation creates a tensor of shape `dims` and fills it with `value`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> tf.fill([2, 3], 9)\n",
      "    <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      "    array([[9, 9, 9],\n",
      "           [9, 9, 9]], dtype=int32)>\n",
      "    \n",
      "    `tf.fill` evaluates at graph runtime and supports dynamic shapes based on\n",
      "    other runtime `tf.Tensors`, unlike `tf.constant(value, shape=dims)`, which\n",
      "    embeds the value as a `Const` node.\n",
      "    \n",
      "    Args:\n",
      "      dims: A 1-D sequence of non-negative numbers. Represents the shape of the\n",
      "        output `tf.Tensor`. Entries should be of type: `int32`, `int64`.\n",
      "      value: A value to fill the returned `tf.Tensor`.\n",
      "      name: Optional string. The name of the output `tf.Tensor`.\n",
      "    \n",
      "    Returns:\n",
      "      A `tf.Tensor` with shape `dims` and the same dtype as `value`.\n",
      "    \n",
      "    Raises:\n",
      "      InvalidArgumentError: `dims` contains negative entries.\n",
      "      NotFoundError: `dims` contains non-integer entries.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Similar to `np.full`. In `numpy`, more parameters are supported. Passing a\n",
      "    number argument as the shape (`np.full(5, value)`) is valid in `numpy` for\n",
      "    specifying a 1-D shaped result, while TensorFlow does not support this syntax.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_d = tf.fill(dims=[3,3],value=8) #Matriz de 3x3, donde todos los valores son 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's is the output of tf.fill(shape=[3,3],8)\n",
    "\n",
    "1. <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
    "array([[1., 1., 1.],\n",
    "       [1., 1., 1.]], dtype=float32)>\n",
    "       \n",
    "2. <tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
    "array([[8, 8, 8],\n",
    "       [8, 8, 8],\n",
    "       [8, 8, 8]])>\n",
    "       \n",
    "3. Sintax error\n",
    "       \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[8, 8, 8],\n",
       "       [8, 8, 8],\n",
       "       [8, 8, 8]])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.random \n",
    "* tf.random.uniform()\n",
    "* tf.random.normal()\n",
    "* tf.random.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function random_normal in module tensorflow.python.ops.random_ops:\n",
      "\n",
      "random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
      "    Outputs random values from a normal distribution.\n",
      "    \n",
      "    Example that generates a new set of random values every time:\n",
      "    \n",
      "    >>> tf.random.set_seed(5);\n",
      "    >>> tf.random.normal([4], 0, 1, tf.float32)\n",
      "    <tf.Tensor: shape=(4,), dtype=float32, numpy=..., dtype=float32)>\n",
      "    \n",
      "    Example that outputs a reproducible result:\n",
      "    \n",
      "    >>> tf.random.set_seed(5);\n",
      "    >>> tf.random.normal([2,2], 0, 1, tf.float32, seed=1)\n",
      "    <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
      "    array([[-1.3768897 , -0.01258316],\n",
      "          [-0.169515   ,  1.0824056 ]], dtype=float32)>\n",
      "    \n",
      "    In this case, we are setting both the global and operation-level seed to\n",
      "    ensure this result is reproducible.  See `tf.random.set_seed` for more\n",
      "    information.\n",
      "    \n",
      "    Args:\n",
      "      shape: A 1-D integer Tensor or Python array. The shape of the output tensor.\n",
      "      mean: A Tensor or Python value of type `dtype`, broadcastable with `stddev`.\n",
      "        The mean of the normal distribution.\n",
      "      stddev: A Tensor or Python value of type `dtype`, broadcastable with `mean`.\n",
      "        The standard deviation of the normal distribution.\n",
      "      dtype: The type of the output.\n",
      "      seed: A Python integer. Used to create a random seed for the distribution.\n",
      "        See\n",
      "        `tf.random.set_seed`\n",
      "        for behavior.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A tensor of the specified shape filled with random normal values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.random.normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_e = tf.random.normal([5,5],seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[-0.8113182 ,  1.4845988 ,  0.06532937, -2.4427042 ,  0.0992484 ],\n",
       "       [ 0.5912243 ,  0.59282297, -2.1229296 , -0.72289723, -0.05627038],\n",
       "       [ 0.6435448 , -0.26432407,  1.8566332 ,  0.5678417 , -0.3828359 ],\n",
       "       [-1.4853433 ,  1.2617711 , -0.02530608, -0.2646297 ,  1.5328138 ],\n",
       "       [-1.7429771 , -0.43789294, -0.56601   ,  0.32066926,  1.132831  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function convert_to_tensor_v2 in module tensorflow.python.framework.ops:\n",
      "\n",
      "convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None)\n",
      "    Converts the given `value` to a `Tensor`.\n",
      "    \n",
      "    This function converts Python objects of various types to `Tensor`\n",
      "    objects. It accepts `Tensor` objects, numpy arrays, Python lists,\n",
      "    and Python scalars. For example:\n",
      "    \n",
      "    >>> def my_func(arg):\n",
      "    ...   arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n",
      "    ...   return arg\n",
      "    \n",
      "    >>> # The following calls are equivalent.\n",
      "    >>> value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))\n",
      "    >>> print(value_1)\n",
      "    tf.Tensor(\n",
      "      [[1. 2.]\n",
      "       [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "    >>> value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])\n",
      "    >>> print(value_2)\n",
      "    tf.Tensor(\n",
      "      [[1. 2.]\n",
      "       [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "    >>> value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))\n",
      "    >>> print(value_3)\n",
      "    tf.Tensor(\n",
      "      [[1. 2.]\n",
      "       [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "    \n",
      "    This function can be useful when composing a new operation in Python\n",
      "    (such as `my_func` in the example above). All standard Python op\n",
      "    constructors apply this function to each of their Tensor-valued\n",
      "    inputs, which allows those ops to accept numpy arrays, Python lists,\n",
      "    and scalars in addition to `Tensor` objects.\n",
      "    \n",
      "    Note: This function diverges from default Numpy behavior for `float` and\n",
      "      `string` types when `None` is present in a Python list or scalar. Rather\n",
      "      than silently converting `None` values, an error will be thrown.\n",
      "    \n",
      "    Args:\n",
      "      value: An object whose type has a registered `Tensor` conversion function.\n",
      "      dtype: Optional element type for the returned tensor. If missing, the type\n",
      "        is inferred from the type of `value`.\n",
      "      dtype_hint: Optional element type for the returned tensor, used when dtype\n",
      "        is None. In some cases, a caller may not have a dtype in mind when\n",
      "        converting to a tensor, so dtype_hint can be used as a soft preference.\n",
      "        If the conversion to `dtype_hint` is not possible, this argument has no\n",
      "        effect.\n",
      "      name: Optional name to use if a new `Tensor` is created.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` based on `value`.\n",
      "    \n",
      "    Raises:\n",
      "      TypeError: If no conversion function is registered for `value` to `dtype`.\n",
      "      RuntimeError: If a registered conversion function returns an invalid value.\n",
      "      ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.convert_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listExample7 = [1,2,3,4,5,6]\n",
    "type(listExample7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2tensor = tf.convert_to_tensor(listExample7,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 4., 5., 6.], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensores variables \n",
    "\n",
    "tf.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Variable in module tensorflow.python.ops.variables:\n",
      "\n",
      "class Variable(tensorflow.python.training.tracking.base.Trackable)\n",
      " |  Variable(*args, **kwargs)\n",
      " |  \n",
      " |  See the [variable guide](https://tensorflow.org/guide/variable).\n",
      " |  \n",
      " |  A variable maintains shared, persistent state manipulated by a program.\n",
      " |  \n",
      " |  The `Variable()` constructor requires an initial value for the variable, which\n",
      " |  can be a `Tensor` of any type and shape. This initial value defines the type\n",
      " |  and shape of the variable. After construction, the type and shape of the\n",
      " |  variable are fixed. The value can be changed using one of the assign methods.\n",
      " |  \n",
      " |  >>> v = tf.Variable(1.)\n",
      " |  >>> v.assign(2.)\n",
      " |  <tf.Variable ... shape=() dtype=float32, numpy=2.0>\n",
      " |  >>> v.assign_add(0.5)\n",
      " |  <tf.Variable ... shape=() dtype=float32, numpy=2.5>\n",
      " |  \n",
      " |  The `shape` argument to `Variable`'s constructor allows you to construct a\n",
      " |  variable with a less defined shape than its `initial_value`:\n",
      " |  \n",
      " |  >>> v = tf.Variable(1., shape=tf.TensorShape(None))\n",
      " |  >>> v.assign([[1.]])\n",
      " |  <tf.Variable ... shape=<unknown> dtype=float32, numpy=array([[1.]], ...)>\n",
      " |  \n",
      " |  Just like any `Tensor`, variables created with `Variable()` can be used as\n",
      " |  inputs to operations. Additionally, all the operators overloaded for the\n",
      " |  `Tensor` class are carried over to variables.\n",
      " |  \n",
      " |  >>> w = tf.Variable([[1.], [2.]])\n",
      " |  >>> x = tf.constant([[3., 4.]])\n",
      " |  >>> tf.matmul(w, x)\n",
      " |  <tf.Tensor:... shape=(2, 2), ... numpy=\n",
      " |    array([[3., 4.],\n",
      " |           [6., 8.]], dtype=float32)>\n",
      " |  >>> tf.sigmoid(w + x)\n",
      " |  <tf.Tensor:... shape=(2, 2), ...>\n",
      " |  \n",
      " |  When building a machine learning model it is often convenient to distinguish\n",
      " |  between variables holding trainable model parameters and other variables such\n",
      " |  as a `step` variable used to count training steps. To make this easier, the\n",
      " |  variable constructor supports a `trainable=<bool>`\n",
      " |  parameter. `tf.GradientTape` watches trainable variables by default:\n",
      " |  \n",
      " |  >>> with tf.GradientTape(persistent=True) as tape:\n",
      " |  ...   trainable = tf.Variable(1.)\n",
      " |  ...   non_trainable = tf.Variable(2., trainable=False)\n",
      " |  ...   x1 = trainable * 2.\n",
      " |  ...   x2 = non_trainable * 3.\n",
      " |  >>> tape.gradient(x1, trainable)\n",
      " |  <tf.Tensor:... shape=(), dtype=float32, numpy=2.0>\n",
      " |  >>> assert tape.gradient(x2, non_trainable) is None  # Unwatched\n",
      " |  \n",
      " |  Variables are automatically tracked when assigned to attributes of types\n",
      " |  inheriting from `tf.Module`.\n",
      " |  \n",
      " |  >>> m = tf.Module()\n",
      " |  >>> m.v = tf.Variable([1.])\n",
      " |  >>> m.trainable_variables\n",
      " |  (<tf.Variable ... shape=(1,) ... numpy=array([1.], dtype=float32)>,)\n",
      " |  \n",
      " |  This tracking then allows saving variable values to\n",
      " |  [training checkpoints](https://www.tensorflow.org/guide/checkpoint), or to\n",
      " |  [SavedModels](https://www.tensorflow.org/guide/saved_model) which include\n",
      " |  serialized TensorFlow graphs.\n",
      " |  \n",
      " |  Variables are often captured and manipulated by `tf.function`s. This works the\n",
      " |  same way the un-decorated function would have:\n",
      " |  \n",
      " |  >>> v = tf.Variable(0.)\n",
      " |  >>> read_and_decrement = tf.function(lambda: v.assign_sub(0.1))\n",
      " |  >>> read_and_decrement()\n",
      " |  <tf.Tensor: shape=(), dtype=float32, numpy=-0.1>\n",
      " |  >>> read_and_decrement()\n",
      " |  <tf.Tensor: shape=(), dtype=float32, numpy=-0.2>\n",
      " |  \n",
      " |  Variables created inside a `tf.function` must be owned outside the function\n",
      " |  and be created only once:\n",
      " |  \n",
      " |  >>> class M(tf.Module):\n",
      " |  ...   @tf.function\n",
      " |  ...   def __call__(self, x):\n",
      " |  ...     if not hasattr(self, \"v\"):  # Or set self.v to None in __init__\n",
      " |  ...       self.v = tf.Variable(x)\n",
      " |  ...     return self.v * x\n",
      " |  >>> m = M()\n",
      " |  >>> m(2.)\n",
      " |  <tf.Tensor: shape=(), dtype=float32, numpy=4.0>\n",
      " |  >>> m(3.)\n",
      " |  <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n",
      " |  >>> m.v\n",
      " |  <tf.Variable ... shape=() dtype=float32, numpy=2.0>\n",
      " |  \n",
      " |  See the `tf.function` documentation for details.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Variable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ = abs(x, name=None)\n",
      " |      Computes the absolute value of a tensor.\n",
      " |      \n",
      " |      Given a tensor of integer or floating-point values, this operation returns a\n",
      " |      tensor of the same type, where each element contains the absolute value of the\n",
      " |      corresponding element in the input.\n",
      " |      \n",
      " |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
      " |      `float32` or `float64` that is the absolute value of each element in `x`. For\n",
      " |      a complex number \\\\(a + bj\\\\), its absolute value is computed as \\\\(\\sqrt{a^2\n",
      " |      + b^2}\\\\).  For example:\n",
      " |      \n",
      " |      >>> x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
      " |      >>> tf.abs(x)\n",
      " |      <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
      " |      array([[5.25594901],\n",
      " |             [6.60492241]])>\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
      " |          `int32`, `int64`, `complex64` or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` or `SparseTensor` of the same size, type and sparsity as `x`,\n",
      " |          with absolute values. Note, for `complex64` or `complex128` input, the\n",
      " |          returned `Tensor` will be of type `float32` or `float64`, respectively.\n",
      " |  \n",
      " |  __add__ = binary_op_wrapper(x, y)\n",
      " |      The operation invoked by the `Tensor.__add__` operator.\n",
      " |      \n",
      " |        Purpose in the API:\n",
      " |      \n",
      " |          This method is exposed in TensorFlow's API so that library developers\n",
      " |          can register dispatching for `Tensor.__add__` to allow it to handle\n",
      " |          custom composite tensors & other custom objects.\n",
      " |      \n",
      " |          The API symbol is not intended to be called by users directly and does\n",
      " |          appear in TensorFlow's generated documentation.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: The left-hand side of the `+` operator.\n",
      " |        y: The right-hand side of the `+` operator.\n",
      " |        name: an optional name for the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The result of the elementwise `+` operation.\n",
      " |  \n",
      " |  __and__ = binary_op_wrapper(x, y)\n",
      " |      Logical AND function.\n",
      " |      \n",
      " |      The operation works for the following input types:\n",
      " |      \n",
      " |      - Two single elements of type `bool`\n",
      " |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
      " |        be calculated by applying logical AND with the single element to each\n",
      " |        element in the larger Tensor.\n",
      " |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
      " |        the result will be the element-wise logical AND of the two input tensors.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      >>> a = tf.constant([True])\n",
      " |      >>> b = tf.constant([False])\n",
      " |      >>> tf.math.logical_and(a, b)\n",
      " |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>\n",
      " |      \n",
      " |      >>> c = tf.constant([True])\n",
      " |      >>> x = tf.constant([False, True, True, False])\n",
      " |      >>> tf.math.logical_and(c, x)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
      " |      \n",
      " |      >>> y = tf.constant([False, False, True, True])\n",
      " |      >>> z = tf.constant([False, True, False, True])\n",
      " |      >>> tf.math.logical_and(y, z)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False,  True])>\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `tf.Tensor` type bool.\n",
      " |          y: A `tf.Tensor` of type bool.\n",
      " |          name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  __div__ = binary_op_wrapper(x, y)\n",
      " |      Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Deprecated in favor of operator or tf.math.divide.\n",
      " |      \n",
      " |      NOTE: Prefer using the Tensor division operator or tf.divide which obey Python\n",
      " |      3 division operator semantics.\n",
      " |      \n",
      " |      This function divides `x` and `y`, forcing Python 2 semantics. That is, if `x`\n",
      " |      and `y` are both integers then the result will be an integer. This is in\n",
      " |      contrast to Python 3, where division with `/` is always a float while division\n",
      " |      with `//` is always an integer.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Compares two variables element-wise for equality.\n",
      " |  \n",
      " |  __floordiv__ = binary_op_wrapper(x, y)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
      " |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __ge__ = greater_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x >= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6, 7])\n",
      " |      y = tf.constant([5, 2, 5, 10])\n",
      " |      tf.math.greater_equal(x, y) ==> [True, True, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6, 7])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.greater_equal(x, y) ==> [True, False, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __getitem__ = _SliceHelperVar(var, slice_spec)\n",
      " |      Creates a slice helper object given a variable.\n",
      " |      \n",
      " |      This allows creating a sub-tensor from part of the current contents\n",
      " |      of a variable. See `tf.Tensor.__getitem__` for detailed examples\n",
      " |      of slicing.\n",
      " |      \n",
      " |      This function in addition also allows assignment to a sliced range.\n",
      " |      This is similar to `__setitem__` functionality in Python. However,\n",
      " |      the syntax is different so that the user can capture the assignment\n",
      " |      operation for grouping or passing to `sess.run()`.\n",
      " |      For example,\n",
      " |      \n",
      " |      ```python\n",
      " |      import tensorflow as tf\n",
      " |      A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)\n",
      " |      with tf.compat.v1.Session() as sess:\n",
      " |        sess.run(tf.compat.v1.global_variables_initializer())\n",
      " |        print(sess.run(A[:2, :2]))  # => [[1,2], [4,5]]\n",
      " |      \n",
      " |        op = A[:2,:2].assign(22. * tf.ones((2, 2)))\n",
      " |        print(sess.run(op))  # => [[22, 22, 3], [22, 22, 6], [7,8,9]]\n",
      " |      ```\n",
      " |      \n",
      " |      Note that assignments currently do not support NumPy broadcasting\n",
      " |      semantics.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: An `ops.Variable` object.\n",
      " |        slice_spec: The arguments to `Tensor.__getitem__`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
      " |        As an operator. The operator also has a `assign()` method\n",
      " |        that can be used to generate an assignment operator.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a slice range is negative size.\n",
      " |        TypeError: TypeError: If the slice indices aren't int, slice,\n",
      " |          ellipsis, tf.newaxis or int32/int64 tensors.\n",
      " |  \n",
      " |  __gt__ = greater(x, y, name=None)\n",
      " |      Returns the truth value of (x > y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 2, 5])\n",
      " |      tf.math.greater(x, y) ==> [False, True, True]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.greater(x, y) ==> [False, False, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, initial_value=None, trainable=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, import_scope=None, constraint=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, shape=None)\n",
      " |      Creates a new variable with value `initial_value`. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(caching_device)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      A variable's value can be manually cached by calling tf.Variable.read_value() under a tf.device scope. The caching_device argument does not work properly.\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_value: A `Tensor`, or Python object convertible to a `Tensor`,\n",
      " |          which is the initial value for the Variable. The initial value must have\n",
      " |          a shape specified unless `validate_shape` is set to False. Can also be a\n",
      " |          callable with no argument that returns the initial value when called. In\n",
      " |          that case, `dtype` must be specified. (Note that initializer functions\n",
      " |          from init_ops.py must first be bound to a shape before being used here.)\n",
      " |        trainable: If `True`, GradientTapes automatically watch uses of this\n",
      " |          variable. Defaults to `True`, unless `synchronization` is set to\n",
      " |          `ON_READ`, in which case it defaults to `False`.\n",
      " |        validate_shape: If `False`, allows the variable to be initialized with a\n",
      " |          value of unknown shape. If `True`, the default, the shape of\n",
      " |          `initial_value` must be known.\n",
      " |        caching_device: Optional device string describing where the Variable\n",
      " |          should be cached for reading.  Defaults to the Variable's device. If not\n",
      " |          `None`, caches on another device.  Typical use is to cache on the device\n",
      " |          where the Ops using the Variable reside, to deduplicate copying through\n",
      " |          `Switch` and other conditional statements.\n",
      " |        name: Optional name for the variable. Defaults to `'Variable'` and gets\n",
      " |          uniquified automatically.\n",
      " |        variable_def: `VariableDef` protocol buffer. If not `None`, recreates the\n",
      " |          Variable object with its contents, referencing the variable's nodes in\n",
      " |          the graph, which must already exist. The graph is not changed.\n",
      " |          `variable_def` and the other arguments are mutually exclusive.\n",
      " |        dtype: If set, initial_value will be converted to the given type. If\n",
      " |          `None`, either the datatype will be kept (if `initial_value` is a\n",
      " |          Tensor), or `convert_to_tensor` will decide.\n",
      " |        import_scope: Optional `string`. Name scope to add to the `Variable.` Only\n",
      " |          used when initializing from protocol buffer.\n",
      " |        constraint: An optional projection function to be applied to the variable\n",
      " |          after being updated by an `Optimizer` (e.g. used to implement norm\n",
      " |          constraints or value constraints for layer weights). The function must\n",
      " |          take as input the unprojected Tensor representing the value of the\n",
      " |          variable and return the Tensor for the projected value (which must have\n",
      " |          the same shape). Constraints are not safe to use when doing asynchronous\n",
      " |          distributed training.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses when to\n",
      " |          synchronize.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        shape: (optional) The shape of this variable. If None, the shape of\n",
      " |          `initial_value` will be used. When setting this argument to\n",
      " |          `tf.TensorShape(None)` (representing an unspecified shape), the variable\n",
      " |          can be assigned with values of different shapes.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If both `variable_def` and initial_value are specified.\n",
      " |        ValueError: If the initial value is not specified, or does not have a\n",
      " |          shape and `validate_shape` is `True`.\n",
      " |  \n",
      " |  __invert__ = logical_not(x, name=None)\n",
      " |      Returns the truth value of `NOT x` element-wise.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> tf.math.logical_not(tf.constant([True, False]))\n",
      " |      <tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`. A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Dummy method to prevent iteration.\n",
      " |      \n",
      " |      Do not call.\n",
      " |      \n",
      " |      NOTE(mrry): If we register __getitem__ as an overloaded operator,\n",
      " |      Python will valiantly attempt to iterate over the variable's Tensor from 0\n",
      " |      to infinity.  Declaring this method prevents this unintended behavior.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: when invoked.\n",
      " |  \n",
      " |  __le__ = less_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x <= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.less_equal(x, y) ==> [True, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 6, 6])\n",
      " |      tf.math.less_equal(x, y) ==> [True, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __lt__ = less(x, y, name=None)\n",
      " |      Returns the truth value of (x < y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5])\n",
      " |      tf.math.less(x, y) ==> [False, True, False]\n",
      " |      \n",
      " |      x = tf.constant([5, 4, 6])\n",
      " |      y = tf.constant([5, 6, 7])\n",
      " |      tf.math.less(x, y) ==> [False, True, True]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __matmul__ = binary_op_wrapper(x, y)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
      " |      and any further outer dimensions specify matching batch size.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      A simple 2-D tensor matrix multiplication:\n",
      " |      \n",
      " |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      >>> a  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      " |      array([[1, 2, 3],\n",
      " |             [4, 5, 6]], dtype=int32)>\n",
      " |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      >>> b  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      " |      array([[ 7,  8],\n",
      " |             [ 9, 10],\n",
      " |             [11, 12]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
      " |      array([[ 58,  64],\n",
      " |             [139, 154]], dtype=int32)>\n",
      " |      \n",
      " |      A batch matrix multiplication with batch shape [2]:\n",
      " |      \n",
      " |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
      " |      >>> a  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
      " |      array([[[ 1,  2,  3],\n",
      " |              [ 4,  5,  6]],\n",
      " |             [[ 7,  8,  9],\n",
      " |              [10, 11, 12]]], dtype=int32)>\n",
      " |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
      " |      >>> b  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      " |      array([[[13, 14],\n",
      " |              [15, 16],\n",
      " |              [17, 18]],\n",
      " |             [[19, 20],\n",
      " |              [21, 22],\n",
      " |              [23, 24]]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
      " |      array([[[ 94, 100],\n",
      " |              [229, 244]],\n",
      " |             [[508, 532],\n",
      " |              [697, 730]]], dtype=int32)>\n",
      " |      \n",
      " |      Since python >= 3.5 the @ operator is supported\n",
      " |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
      " |      it simply calls the `tf.matmul()` function, so the following lines are\n",
      " |      equivalent:\n",
      " |      \n",
      " |      >>> d = a @ b @ [[10], [11]]\n",
      " |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
      " |          `complex64`, `complex128` and rank > 1.\n",
      " |        b: `tf.Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.sparse.SparseTensor` multiplication.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.sparse.SparseTensor` multiplication.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
      " |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
      " |        for all indices `i`, `j`.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
      " |          `adjoint_b` are both set to `True`.\n",
      " |  \n",
      " |  __mod__ = binary_op_wrapper(x, y)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `uint64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __mul__ = binary_op_wrapper(x, y)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Compares two variables element-wise for equality.\n",
      " |  \n",
      " |  __neg__ = neg(x, name=None)\n",
      " |      Computes numerical negative value element-wise.\n",
      " |      \n",
      " |      I.e., \\\\(y = -x\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __or__ = binary_op_wrapper(x, y)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __pow__ = binary_op_wrapper(x, y)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __radd__ = r_binary_op_wrapper(y, x)\n",
      " |      The operation invoked by the `Tensor.__add__` operator.\n",
      " |      \n",
      " |        Purpose in the API:\n",
      " |      \n",
      " |          This method is exposed in TensorFlow's API so that library developers\n",
      " |          can register dispatching for `Tensor.__add__` to allow it to handle\n",
      " |          custom composite tensors & other custom objects.\n",
      " |      \n",
      " |          The API symbol is not intended to be called by users directly and does\n",
      " |          appear in TensorFlow's generated documentation.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: The left-hand side of the `+` operator.\n",
      " |        y: The right-hand side of the `+` operator.\n",
      " |        name: an optional name for the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The result of the elementwise `+` operation.\n",
      " |  \n",
      " |  __rand__ = r_binary_op_wrapper(y, x)\n",
      " |      Logical AND function.\n",
      " |      \n",
      " |      The operation works for the following input types:\n",
      " |      \n",
      " |      - Two single elements of type `bool`\n",
      " |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
      " |        be calculated by applying logical AND with the single element to each\n",
      " |        element in the larger Tensor.\n",
      " |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
      " |        the result will be the element-wise logical AND of the two input tensors.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      >>> a = tf.constant([True])\n",
      " |      >>> b = tf.constant([False])\n",
      " |      >>> tf.math.logical_and(a, b)\n",
      " |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>\n",
      " |      \n",
      " |      >>> c = tf.constant([True])\n",
      " |      >>> x = tf.constant([False, True, True, False])\n",
      " |      >>> tf.math.logical_and(c, x)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
      " |      \n",
      " |      >>> y = tf.constant([False, False, True, True])\n",
      " |      >>> z = tf.constant([False, True, False, True])\n",
      " |      >>> tf.math.logical_and(y, z)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False,  True])>\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `tf.Tensor` type bool.\n",
      " |          y: A `tf.Tensor` of type bool.\n",
      " |          name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  __rdiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Deprecated in favor of operator or tf.math.divide.\n",
      " |      \n",
      " |      NOTE: Prefer using the Tensor division operator or tf.divide which obey Python\n",
      " |      3 division operator semantics.\n",
      " |      \n",
      " |      This function divides `x` and `y`, forcing Python 2 semantics. That is, if `x`\n",
      " |      and `y` are both integers then the result will be an integer. This is in\n",
      " |      contrast to Python 3, where division with `/` is always a float while division\n",
      " |      with `//` is always an integer.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
      " |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __rmatmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
      " |      and any further outer dimensions specify matching batch size.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      A simple 2-D tensor matrix multiplication:\n",
      " |      \n",
      " |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      >>> a  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      " |      array([[1, 2, 3],\n",
      " |             [4, 5, 6]], dtype=int32)>\n",
      " |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      >>> b  # 2-D tensor\n",
      " |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      " |      array([[ 7,  8],\n",
      " |             [ 9, 10],\n",
      " |             [11, 12]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
      " |      array([[ 58,  64],\n",
      " |             [139, 154]], dtype=int32)>\n",
      " |      \n",
      " |      A batch matrix multiplication with batch shape [2]:\n",
      " |      \n",
      " |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
      " |      >>> a  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
      " |      array([[[ 1,  2,  3],\n",
      " |              [ 4,  5,  6]],\n",
      " |             [[ 7,  8,  9],\n",
      " |              [10, 11, 12]]], dtype=int32)>\n",
      " |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
      " |      >>> b  # 3-D tensor\n",
      " |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      " |      array([[[13, 14],\n",
      " |              [15, 16],\n",
      " |              [17, 18]],\n",
      " |             [[19, 20],\n",
      " |              [21, 22],\n",
      " |              [23, 24]]], dtype=int32)>\n",
      " |      >>> c = tf.matmul(a, b)\n",
      " |      >>> c  # `a` * `b`\n",
      " |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
      " |      array([[[ 94, 100],\n",
      " |              [229, 244]],\n",
      " |             [[508, 532],\n",
      " |              [697, 730]]], dtype=int32)>\n",
      " |      \n",
      " |      Since python >= 3.5 the @ operator is supported\n",
      " |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
      " |      it simply calls the `tf.matmul()` function, so the following lines are\n",
      " |      equivalent:\n",
      " |      \n",
      " |      >>> d = a @ b @ [[10], [11]]\n",
      " |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
      " |          `complex64`, `complex128` and rank > 1.\n",
      " |        b: `tf.Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.sparse.SparseTensor` multiplication.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
      " |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      " |          that assume most values in `a` are zero.\n",
      " |          See `tf.sparse.sparse_dense_matmul`\n",
      " |          for some support for `tf.sparse.SparseTensor` multiplication.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
      " |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
      " |        for all indices `i`, `j`.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
      " |          `adjoint_b` are both set to `True`.\n",
      " |  \n",
      " |  __rmod__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `uint64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ror__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rpow__ = r_binary_op_wrapper(y, x)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __rsub__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `uint32`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rtruediv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divides x / y elementwise (using Python 3 division operator semantics).\n",
      " |      \n",
      " |      NOTE: Prefer using the Tensor operator or tf.divide which obey Python\n",
      " |      division operator semantics.\n",
      " |      \n",
      " |      This function forces Python 3 division operator semantics where all integer\n",
      " |      arguments are cast to floating types first.   This op is generated by normal\n",
      " |      `x / y` division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.  If you want integer division that rounds\n",
      " |      down, use `x // y` or `tf.math.floordiv`.\n",
      " |      \n",
      " |      `x` and `y` must have the same numeric type.  If the inputs are floating\n",
      " |      point, the output will have the same type.  If the inputs are integral, the\n",
      " |      inputs are cast to `float32` for `int8` and `int16` and `float64` for `int32`\n",
      " |      and `int64` (matching the behavior of Numpy).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of numeric type.\n",
      " |        y: `Tensor` denominator of numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` evaluated in floating point.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `x` and `y` have different dtypes.\n",
      " |  \n",
      " |  __rxor__ = r_binary_op_wrapper(y, x)\n",
      " |      Logical XOR function.\n",
      " |      \n",
      " |      x ^ y = (x | y) & ~(x & y)\n",
      " |      \n",
      " |      The operation works for the following input types:\n",
      " |      \n",
      " |      - Two single elements of type `bool`\n",
      " |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
      " |        be calculated by applying logical XOR with the single element to each\n",
      " |        element in the larger Tensor.\n",
      " |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
      " |        the result will be the element-wise logical XOR of the two input tensors.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      >>> a = tf.constant([True])\n",
      " |      >>> b = tf.constant([False])\n",
      " |      >>> tf.math.logical_xor(a, b)\n",
      " |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
      " |      \n",
      " |      >>> c = tf.constant([True])\n",
      " |      >>> x = tf.constant([False, True, True, False])\n",
      " |      >>> tf.math.logical_xor(c, x)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
      " |      \n",
      " |      >>> y = tf.constant([False, False, True, True])\n",
      " |      >>> z = tf.constant([False, True, False, True])\n",
      " |      >>> tf.math.logical_xor(y, z)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `tf.Tensor` type bool.\n",
      " |          y: A `tf.Tensor` of type bool.\n",
      " |          name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  __sub__ = binary_op_wrapper(x, y)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `uint32`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __truediv__ = binary_op_wrapper(x, y)\n",
      " |      Divides x / y elementwise (using Python 3 division operator semantics).\n",
      " |      \n",
      " |      NOTE: Prefer using the Tensor operator or tf.divide which obey Python\n",
      " |      division operator semantics.\n",
      " |      \n",
      " |      This function forces Python 3 division operator semantics where all integer\n",
      " |      arguments are cast to floating types first.   This op is generated by normal\n",
      " |      `x / y` division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.  If you want integer division that rounds\n",
      " |      down, use `x // y` or `tf.math.floordiv`.\n",
      " |      \n",
      " |      `x` and `y` must have the same numeric type.  If the inputs are floating\n",
      " |      point, the output will have the same type.  If the inputs are integral, the\n",
      " |      inputs are cast to `float32` for `int8` and `int16` and `float64` for `int32`\n",
      " |      and `int64` (matching the behavior of Numpy).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of numeric type.\n",
      " |        y: `Tensor` denominator of numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` evaluated in floating point.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `x` and `y` have different dtypes.\n",
      " |  \n",
      " |  __xor__ = binary_op_wrapper(x, y)\n",
      " |      Logical XOR function.\n",
      " |      \n",
      " |      x ^ y = (x | y) & ~(x & y)\n",
      " |      \n",
      " |      The operation works for the following input types:\n",
      " |      \n",
      " |      - Two single elements of type `bool`\n",
      " |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
      " |        be calculated by applying logical XOR with the single element to each\n",
      " |        element in the larger Tensor.\n",
      " |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
      " |        the result will be the element-wise logical XOR of the two input tensors.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      >>> a = tf.constant([True])\n",
      " |      >>> b = tf.constant([False])\n",
      " |      >>> tf.math.logical_xor(a, b)\n",
      " |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
      " |      \n",
      " |      >>> c = tf.constant([True])\n",
      " |      >>> x = tf.constant([False, True, True, False])\n",
      " |      >>> tf.math.logical_xor(c, x)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
      " |      \n",
      " |      >>> y = tf.constant([False, False, True, True])\n",
      " |      >>> z = tf.constant([False, True, False, True])\n",
      " |      >>> tf.math.logical_xor(y, z)\n",
      " |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `tf.Tensor` type bool.\n",
      " |          y: A `tf.Tensor` of type bool.\n",
      " |          name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  assign(self, value, use_locking=False, name=None, read_value=True)\n",
      " |      Assigns a new value to the variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign(self, value)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: A `Tensor`. The new value for this variable.\n",
      " |        use_locking: If `True`, use locking during the assignment.\n",
      " |        name: The name of the operation to be created\n",
      " |        read_value: if True, will return something which evaluates to the new\n",
      " |          value of the variable; if False will return the assign op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable. If `read_value` is false, instead returns None in\n",
      " |        Eager mode and the assign op in graph mode.\n",
      " |  \n",
      " |  assign_add(self, delta, use_locking=False, name=None, read_value=True)\n",
      " |      Adds a value to this variable.\n",
      " |      \n",
      " |       This is essentially a shortcut for `assign_add(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to add to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: The name of the operation to be created\n",
      " |        read_value: if True, will return something which evaluates to the new\n",
      " |          value of the variable; if False will return the assign op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable. If `read_value` is false, instead returns None in\n",
      " |        Eager mode and the assign op in graph mode.\n",
      " |  \n",
      " |  assign_sub(self, delta, use_locking=False, name=None, read_value=True)\n",
      " |      Subtracts a value from this variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign_sub(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to subtract from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: The name of the operation to be created\n",
      " |        read_value: if True, will return something which evaluates to the new\n",
      " |          value of the variable; if False will return the assign op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable. If `read_value` is false, instead returns None in\n",
      " |        Eager mode and the assign op in graph mode.\n",
      " |  \n",
      " |  batch_scatter_update(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Assigns `tf.IndexedSlices` to this variable batch-wise.\n",
      " |      \n",
      " |      Analogous to `batch_gather`. This assumes that this variable and the\n",
      " |      sparse_delta IndexedSlices have a series of leading dimensions that are the\n",
      " |      same for all of them, and the updates are performed on the last dimension of\n",
      " |      indices. In other words, the dimensions should be the following:\n",
      " |      \n",
      " |      `num_prefix_dims = sparse_delta.indices.ndims - 1`\n",
      " |      `batch_dim = num_prefix_dims + 1`\n",
      " |      `sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[\n",
      " |           batch_dim:]`\n",
      " |      \n",
      " |      where\n",
      " |      \n",
      " |      `sparse_delta.updates.shape[:num_prefix_dims]`\n",
      " |      `== sparse_delta.indices.shape[:num_prefix_dims]`\n",
      " |      `== var.shape[:num_prefix_dims]`\n",
      " |      \n",
      " |      And the operation performed can be expressed as:\n",
      " |      \n",
      " |      `var[i_1, ..., i_n,\n",
      " |           sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[\n",
      " |              i_1, ..., i_n, j]`\n",
      " |      \n",
      " |      When sparse_delta.indices is a 1D tensor, this operation is equivalent to\n",
      " |      `scatter_update`.\n",
      " |      \n",
      " |      To avoid this operation one can looping over the first `ndims` of the\n",
      " |      variable and using `scatter_update` on the subtensors that result of slicing\n",
      " |      the first dimension. This is a valid option for `ndims = 1`, but less\n",
      " |      efficient than this implementation.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be assigned to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  count_up_to(self, limit)\n",
      " |      Increments this variable until it reaches `limit`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Prefer Dataset.range instead.\n",
      " |      \n",
      " |      When that Op is run it tries to increment the variable by `1`. If\n",
      " |      incrementing the variable would bring it above `limit` then the Op raises\n",
      " |      the exception `OutOfRangeError`.\n",
      " |      \n",
      " |      If no error is raised, the Op outputs the value of the variable before\n",
      " |      the increment.\n",
      " |      \n",
      " |      This is essentially a shortcut for `count_up_to(self, limit)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        limit: value at which incrementing the variable raises an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the variable value before the increment. If no\n",
      " |        other Op modifies this variable, the values produced will all be\n",
      " |        distinct.\n",
      " |  \n",
      " |  eval(self, session=None)\n",
      " |      In a session, computes and returns the value of this variable.\n",
      " |      \n",
      " |      This is not a graph construction method, it does not add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See `tf.compat.v1.Session` for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.compat.v1.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.compat.v1.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          print(v.eval(sess))\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          print(v.eval())\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        session: The session to use to evaluate this variable. If none, the\n",
      " |          default session is used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A numpy `ndarray` with a copy of the value of this variable.\n",
      " |  \n",
      " |  experimental_ref(self)\n",
      " |      DEPRECATED FUNCTION\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use ref() instead.\n",
      " |  \n",
      " |  gather_nd(self, indices, name=None)\n",
      " |      Gather slices from `params` into a Tensor with shape specified by `indices`.\n",
      " |      \n",
      " |      See tf.gather_nd for details.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      " |          Index tensor.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `params`.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Alias of `Variable.shape`.\n",
      " |  \n",
      " |  initialized_value(self)\n",
      " |      Returns the value of the initialized variable. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      " |      \n",
      " |      You should use this instead of the variable itself to initialize another\n",
      " |      variable with a value that depends on the value of this variable.\n",
      " |      \n",
      " |      ```python\n",
      " |      # Initialize 'v' with a random tensor.\n",
      " |      v = tf.Variable(tf.random.truncated_normal([10, 40]))\n",
      " |      # Use `initialized_value` to guarantee that `v` has been\n",
      " |      # initialized before its value is used to initialize `w`.\n",
      " |      # The random values are picked only once.\n",
      " |      w = tf.Variable(v.initialized_value() * 2.0)\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` holding the value of this variable after its initializer\n",
      " |        has run.\n",
      " |  \n",
      " |  load(self, value, session=None)\n",
      " |      Load new value into this variable. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      " |      \n",
      " |      Writes new value to variable's memory. Doesn't add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See `tf.compat.v1.Session` for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.compat.v1.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.compat.v1.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          v.load([2, 3], sess)\n",
      " |          print(v.eval(sess)) # prints [2 3]\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          v.load([3, 4], sess)\n",
      " |          print(v.eval()) # prints [3 4]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          value: New variable value\n",
      " |          session: The session to use to evaluate this variable. If none, the\n",
      " |            default session is used.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: Session is not passed and no default session\n",
      " |  \n",
      " |  read_value(self)\n",
      " |      Returns the value of this variable, read in the current context.\n",
      " |      \n",
      " |      Can be different from value() if it's on another device, with control\n",
      " |      dependencies, etc.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  ref(self)\n",
      " |      Returns a hashable reference object to this Variable.\n",
      " |      \n",
      " |      The primary use case for this API is to put variables in a set/dictionary.\n",
      " |      We can't put variables in a set/dictionary as `variable.__hash__()` is no\n",
      " |      longer available starting Tensorflow 2.0.\n",
      " |      \n",
      " |      The following will raise an exception starting 2.0\n",
      " |      \n",
      " |      >>> x = tf.Variable(5)\n",
      " |      >>> y = tf.Variable(10)\n",
      " |      >>> z = tf.Variable(10)\n",
      " |      >>> variable_set = {x, y, z}\n",
      " |      Traceback (most recent call last):\n",
      " |        ...\n",
      " |      TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.\n",
      " |      >>> variable_dict = {x: 'five', y: 'ten'}\n",
      " |      Traceback (most recent call last):\n",
      " |        ...\n",
      " |      TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.\n",
      " |      \n",
      " |      Instead, we can use `variable.ref()`.\n",
      " |      \n",
      " |      >>> variable_set = {x.ref(), y.ref(), z.ref()}\n",
      " |      >>> x.ref() in variable_set\n",
      " |      True\n",
      " |      >>> variable_dict = {x.ref(): 'five', y.ref(): 'ten', z.ref(): 'ten'}\n",
      " |      >>> variable_dict[y.ref()]\n",
      " |      'ten'\n",
      " |      \n",
      " |      Also, the reference object provides `.deref()` function that returns the\n",
      " |      original Variable.\n",
      " |      \n",
      " |      >>> x = tf.Variable(5)\n",
      " |      >>> x.ref().deref()\n",
      " |      <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5>\n",
      " |  \n",
      " |  scatter_add(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Adds `tf.IndexedSlices` to this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be added to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_div(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Divide this variable by `tf.IndexedSlices`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to divide this variable by.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_max(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Updates this variable with the max of `tf.IndexedSlices` and itself.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to use as an argument of max with this\n",
      " |          variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_min(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Updates this variable with the min of `tf.IndexedSlices` and itself.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to use as an argument of min with this\n",
      " |          variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_mul(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Multiply this variable by `tf.IndexedSlices`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to multiply this variable by.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_nd_add(self, indices, updates, name=None)\n",
      " |      Applies sparse addition to individual values or slices in a Variable.\n",
      " |      \n",
      " |      The Variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into self.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of self.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          add = v.scatter_nd_add(indices, updates)\n",
      " |          with tf.compat.v1.Session() as sess:\n",
      " |            print sess.run(add)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to v would look like this:\n",
      " |      \n",
      " |          [1, 13, 3, 14, 14, 6, 7, 20]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |  \n",
      " |  scatter_nd_sub(self, indices, updates, name=None)\n",
      " |      Applies sparse subtraction to individual values or slices in a Variable.\n",
      " |      \n",
      " |      Assuming the variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into self.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of self.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          op = v.scatter_nd_sub(indices, updates)\n",
      " |          with tf.compat.v1.Session() as sess:\n",
      " |            print sess.run(op)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to v would look like this:\n",
      " |      \n",
      " |          [1, -9, 3, -6, -6, 6, 7, -4]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |  \n",
      " |  scatter_nd_update(self, indices, updates, name=None)\n",
      " |      Applies sparse assignment to individual values or slices in a Variable.\n",
      " |      \n",
      " |      The Variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into self.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of self.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          op = v.scatter_nd_assign(indices, updates)\n",
      " |          with tf.compat.v1.Session() as sess:\n",
      " |            print sess.run(op)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to v would look like this:\n",
      " |      \n",
      " |          [1, 11, 3, 10, 9, 6, 7, 12]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |  \n",
      " |  scatter_sub(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Subtracts `tf.IndexedSlices` from this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be subtracted from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_update(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Assigns `tf.IndexedSlices` to this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `tf.IndexedSlices` to be assigned to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The updated variable.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  set_shape(self, shape)\n",
      " |      Overrides the shape for this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: the `TensorShape` representing the overridden shape.\n",
      " |  \n",
      " |  sparse_read(self, indices, name=None)\n",
      " |      Gather slices from params axis axis according to indices.\n",
      " |      \n",
      " |      This function supports a subset of tf.gather, see tf.gather for details on\n",
      " |      usage.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The index `Tensor`.  Must be one of the following types: `int32`,\n",
      " |          `int64`. Must be in range `[0, params.shape[axis])`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `params`.\n",
      " |  \n",
      " |  to_proto(self, export_scope=None)\n",
      " |      Converts a `Variable` to a `VariableDef` protocol buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |        export_scope: Optional `string`. Name scope to remove.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VariableDef` protocol buffer, or `None` if the `Variable` is not\n",
      " |        in the specified name scope.\n",
      " |  \n",
      " |  value(self)\n",
      " |      Returns the last snapshot of this variable.\n",
      " |      \n",
      " |      You usually do not need to call this method as all ops that need the value\n",
      " |      of the variable call it automatically through a `convert_to_tensor()` call.\n",
      " |      \n",
      " |      Returns a `Tensor` which holds the value of the variable.  You can not\n",
      " |      assign a new value to this tensor as it is not a reference to the variable.\n",
      " |      \n",
      " |      To avoid copies, if the consumer of the returned value is on the same device\n",
      " |      as the variable, this actually returns the live value of the variable, not\n",
      " |      a copy.  Updates to the variable are seen by the consumer.  If the consumer\n",
      " |      is on a different device it will get a copy of the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_proto(variable_def, import_scope=None)\n",
      " |      Returns a `Variable` object created from `variable_def`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  aggregation\n",
      " |  \n",
      " |  constraint\n",
      " |      Returns the constraint function associated with this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The constraint function that was passed to the variable constructor.\n",
      " |        Can be `None` if no constraint was passed.\n",
      " |  \n",
      " |  device\n",
      " |      The device of this variable.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of this variable.\n",
      " |  \n",
      " |  graph\n",
      " |      The `Graph` of this variable.\n",
      " |  \n",
      " |  initial_value\n",
      " |      Returns the Tensor used as the initial value for the variable.\n",
      " |      \n",
      " |      Note that this is different from `initialized_value()` which runs\n",
      " |      the op that initializes the variable before returning its value.\n",
      " |      This method returns the tensor that is used by the op that initializes\n",
      " |      the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  initializer\n",
      " |      The initializer operation for this variable.\n",
      " |  \n",
      " |  name\n",
      " |      The name of this variable.\n",
      " |  \n",
      " |  op\n",
      " |      The `Operation` of this variable.\n",
      " |  \n",
      " |  shape\n",
      " |      The `TensorShape` of this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `TensorShape`.\n",
      " |  \n",
      " |  synchronization\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  SaveSliceInfo = <class 'tensorflow.python.ops.variables.Variable.SaveS...\n",
      " |      Information on how to save this Variable as a slice.\n",
      " |      \n",
      " |      Provides internal support for saving variables as slices of a larger\n",
      " |      variable.  This API is not public and is subject to change.\n",
      " |      \n",
      " |      Available properties:\n",
      " |      \n",
      " |      * full_name\n",
      " |      * full_shape\n",
      " |      * var_offset\n",
      " |      * var_shape\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[1., 1., 1.],\n",
      "       [1., 1., 1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "var_1 = tf.Variable(tf.ones([2,3]))\n",
    "print(var_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la variable var_1 tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('Valor de la variable var_1', var_1.read_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la variable var_1 tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "var_value_1 = [[1,2,3],[4,5,6]]\n",
    "var_1.assign(var_value_1)\n",
    "print('Valor de la variable var_1', var_1.read_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing e Indexing de los tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing \n",
    "* [start:end] - Extrae desde la posición inicial hasta la posición final\n",
    "* [start:end:step] o [::step] - Extrae en un intervalo de pasos\n",
    "* [::-1] - Segmentación de datos del último elemento\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[-1.00054467e+00 -6.52104735e-01  1.09322834e+00]\n",
      "   [ 9.87382114e-01  1.35380030e+00 -1.45619881e+00]\n",
      "   [ 8.31169665e-01  2.27716357e-01  3.20754141e-01]\n",
      "   ...\n",
      "   [ 7.49929130e-01 -2.49788690e+00 -1.78810680e+00]\n",
      "   [ 7.64013886e-01 -1.03356451e-01  5.81838667e-01]\n",
      "   [ 1.22685242e+00  2.40965319e+00 -1.46277726e-01]]\n",
      "\n",
      "  [[-2.62106627e-01  2.94600725e-02 -7.98700333e-01]\n",
      "   [ 5.54642081e-01 -9.55811799e-01  6.64957762e-01]\n",
      "   [-4.39755529e-01 -9.90762532e-01  1.20675361e+00]\n",
      "   ...\n",
      "   [ 1.29253447e-01  2.51268888e+00 -7.37603128e-01]\n",
      "   [-1.56515932e+00 -1.23989537e-01 -1.04234195e+00]\n",
      "   [-7.14586496e-01  5.78985333e-01 -5.27908504e-01]]\n",
      "\n",
      "  [[ 5.92542887e-01 -3.80327851e-01  1.37927508e+00]\n",
      "   [-2.84432173e-01 -1.15593277e-01 -2.15790415e+00]\n",
      "   [ 9.94845152e-01 -4.68406528e-02  1.42201638e+00]\n",
      "   ...\n",
      "   [ 1.13928235e+00 -2.49847651e+00 -9.77994781e-03]\n",
      "   [-1.41256678e+00  1.17498147e+00 -5.46238363e-01]\n",
      "   [-9.96809065e-01  6.51917517e-01 -2.58832550e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.40855622e+00 -7.80569971e-01 -1.39159477e+00]\n",
      "   [ 2.75478315e+00  5.02112687e-01 -2.69917697e-01]\n",
      "   [ 9.89754051e-02  1.16690624e+00  9.82834548e-02]\n",
      "   ...\n",
      "   [-9.74316671e-02 -2.03215688e-01 -5.59393823e-01]\n",
      "   [-1.12672341e+00 -8.53887260e-01  7.04275846e-01]\n",
      "   [-6.22858226e-01 -1.39563298e+00  8.39619756e-01]]\n",
      "\n",
      "  [[-2.86846042e-01  6.74752355e-01  1.10213411e+00]\n",
      "   [ 7.86219358e-01  4.39224631e-01 -1.18097866e+00]\n",
      "   [-1.00176108e+00  5.08414388e-01 -4.00134295e-01]\n",
      "   ...\n",
      "   [ 1.78704977e+00 -2.23391965e-01 -8.22172403e-01]\n",
      "   [ 4.72454391e-02 -2.46239424e-01  2.03402424e+00]\n",
      "   [-7.20662698e-02  2.83927947e-01  6.43173039e-01]]\n",
      "\n",
      "  [[ 2.96761304e-01  1.19781959e+00 -1.07434261e+00]\n",
      "   [ 5.05302288e-02 -1.60994411e+00 -1.94614995e+00]\n",
      "   [ 2.26525500e-01 -7.54154503e-01 -2.31581283e+00]\n",
      "   ...\n",
      "   [ 1.79214263e+00  3.28613997e-01  7.78264582e-01]\n",
      "   [ 5.30458570e-01  3.05241615e-01 -2.01118398e+00]\n",
      "   [ 1.02069688e+00 -7.78501868e-01 -1.16144545e-01]]]\n",
      "\n",
      "\n",
      " [[[-1.50014043e-01 -1.39248478e+00 -6.77798092e-01]\n",
      "   [-1.98917127e+00  1.06064096e-01 -1.30953580e-01]\n",
      "   [ 8.82990479e-01 -3.51989865e-02 -2.73631549e+00]\n",
      "   ...\n",
      "   [ 1.68346196e-01 -1.36832282e-01  1.14109230e+00]\n",
      "   [-6.58138096e-01 -3.51297902e-03 -8.36209208e-02]\n",
      "   [-2.38436174e+00 -6.61761165e-01 -9.58511755e-02]]\n",
      "\n",
      "  [[ 1.48184288e+00  2.08133146e-01  4.21317220e-01]\n",
      "   [ 1.01424563e+00  1.42577159e+00  7.09047318e-01]\n",
      "   [ 1.51224232e+00 -1.06013668e+00  1.38933808e-01]\n",
      "   ...\n",
      "   [-6.02511823e-01 -5.42413533e-01  2.38507628e+00]\n",
      "   [-1.34048775e-01  4.48654979e-01  1.62817419e-01]\n",
      "   [-4.45997685e-01  4.96715039e-01  2.44963431e+00]]\n",
      "\n",
      "  [[-5.28492093e-01  1.01954412e+00  3.71956617e-01]\n",
      "   [-2.35002220e-01 -2.26174027e-01  6.46820426e-01]\n",
      "   [ 5.04460871e-01  4.66103554e-01  5.23691952e-01]\n",
      "   ...\n",
      "   [ 1.32431507e-01 -1.61450422e+00  1.15370817e-01]\n",
      "   [-1.48582697e+00  5.71251214e-01  9.95844901e-01]\n",
      "   [-5.16170859e-02 -2.62882739e-01 -7.34930396e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-9.73277152e-01  1.76439905e+00  6.18197136e-02]\n",
      "   [-1.67513585e+00 -1.04321487e-01  1.68196484e-01]\n",
      "   [-6.17433265e-02 -1.28547415e-01  9.15369153e-01]\n",
      "   ...\n",
      "   [-1.92184007e+00 -2.54262425e-02  1.25219202e+00]\n",
      "   [-1.71559358e+00 -1.39991176e+00  9.04901147e-01]\n",
      "   [-6.18344903e-01  7.78819442e-01  4.18251812e-01]]\n",
      "\n",
      "  [[-8.28820989e-02 -3.67210537e-01  1.98557007e+00]\n",
      "   [ 5.10159135e-01 -2.51047671e-01 -1.67831337e+00]\n",
      "   [ 4.08002794e-01 -1.23062801e+00  6.86083198e-01]\n",
      "   ...\n",
      "   [ 4.99584556e-01 -2.71540880e-01 -5.55180430e-01]\n",
      "   [ 1.65365249e-01 -1.10307872e+00  2.71765041e+00]\n",
      "   [ 4.35609549e-01 -5.90904772e-01  1.29071903e+00]]\n",
      "\n",
      "  [[ 1.42517710e+00 -1.60294187e+00 -9.68891740e-01]\n",
      "   [-2.96124160e-01  1.48607743e+00 -1.16679549e+00]\n",
      "   [-1.14158320e+00  1.52382994e+00  5.82725763e-01]\n",
      "   ...\n",
      "   [-7.41295338e-01  2.81678110e-01  6.39159560e-01]\n",
      "   [ 9.36445177e-01 -9.69879150e-01  1.47812927e+00]\n",
      "   [-1.15554726e+00  9.91636336e-01 -7.70962238e-02]]]\n",
      "\n",
      "\n",
      " [[[-6.28224835e-02 -1.17372847e+00  1.26965022e+00]\n",
      "   [ 9.93977606e-01  1.34311587e-01  1.16499496e+00]\n",
      "   [-2.76419878e+00 -1.00524700e+00  1.01916504e+00]\n",
      "   ...\n",
      "   [-1.07149398e+00 -1.42113447e-01 -8.61548066e-01]\n",
      "   [ 1.99340320e+00  5.07394791e-01  1.01565862e+00]\n",
      "   [ 2.31038675e-01 -1.63555729e+00 -2.83367127e-01]]\n",
      "\n",
      "  [[ 3.60259086e-01  4.93312418e-01 -1.35669482e+00]\n",
      "   [ 3.85756224e-01 -4.35308009e-01  1.84762311e+00]\n",
      "   [-1.03597224e-01  8.01793098e-01  1.10615432e+00]\n",
      "   ...\n",
      "   [ 4.69518900e-01  8.21810544e-01 -8.89651179e-01]\n",
      "   [ 1.11035478e+00  1.76264346e+00 -5.63013971e-01]\n",
      "   [ 4.89291728e-01 -5.08557022e-01 -1.12201309e+00]]\n",
      "\n",
      "  [[ 2.25435901e+00 -7.47746885e-01 -2.23424479e-01]\n",
      "   [-9.52993035e-01 -9.45434943e-02  9.44523692e-01]\n",
      "   [ 5.64283431e-02  4.39294606e-01  9.07467365e-01]\n",
      "   ...\n",
      "   [-1.33613837e+00 -9.72159684e-01  9.90366995e-01]\n",
      "   [ 3.55851948e-01  1.68118608e+00  1.62753165e+00]\n",
      "   [ 2.22460032e+00 -9.75258291e-01  2.42003754e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 4.42894489e-01  2.09332299e+00 -5.99415958e-01]\n",
      "   [-1.01092958e+00 -5.32131910e-01  8.07332933e-01]\n",
      "   [-1.82469919e-01 -5.64218640e-01 -1.86187148e-01]\n",
      "   ...\n",
      "   [-3.85265797e-01 -3.49623442e-01 -4.69022155e-01]\n",
      "   [-6.22611679e-02 -7.37137675e-01 -9.34295833e-01]\n",
      "   [-3.78808007e-02  1.32047459e-01  8.60221922e-01]]\n",
      "\n",
      "  [[-3.33456129e-01 -1.83633184e+00  3.84299606e-01]\n",
      "   [-1.06601095e+00  1.49797010e+00 -4.99462456e-01]\n",
      "   [-2.98765115e-03 -3.25285435e-01 -8.63332093e-01]\n",
      "   ...\n",
      "   [-3.06980461e-01 -3.45553041e-01 -8.59102793e-03]\n",
      "   [ 6.36876225e-01  3.52242142e-01 -5.93421347e-02]\n",
      "   [-1.46574044e+00 -5.88912427e-01  8.91186714e-01]]\n",
      "\n",
      "  [[-1.48137772e+00 -9.04136896e-02  2.82237139e-02]\n",
      "   [-6.31177068e-01  1.81993529e-01 -3.50717992e-01]\n",
      "   [-1.58436096e+00 -4.79797125e-01  7.69526422e-01]\n",
      "   ...\n",
      "   [-1.05943561e+00 -4.61119823e-02  7.21553922e-01]\n",
      "   [-5.19266307e-01 -2.01746011e+00  1.24453291e-01]\n",
      "   [-1.53950679e+00  1.03301656e+00  5.09335637e-01]]]\n",
      "\n",
      "\n",
      " [[[-1.11900032e+00  4.81359005e-01 -9.91551816e-01]\n",
      "   [-1.63031006e+00  1.72501713e-01  8.15835357e-01]\n",
      "   [ 1.59688011e-01  1.33077705e+00  4.56041634e-01]\n",
      "   ...\n",
      "   [ 1.91381383e+00 -1.89852047e+00 -7.55266726e-01]\n",
      "   [-2.84153819e-01  1.40374526e-02  4.47298065e-02]\n",
      "   [ 6.09242141e-01  7.61439949e-02 -2.10788584e+00]]\n",
      "\n",
      "  [[ 7.35600114e-01  4.78352547e-01  8.96837592e-01]\n",
      "   [ 6.47120416e-01 -1.09583867e+00 -1.19604361e+00]\n",
      "   [ 6.66618526e-01  3.98706526e-01  1.80406556e-01]\n",
      "   ...\n",
      "   [ 9.84750032e-01 -1.97311834e-01 -3.01506847e-01]\n",
      "   [ 3.38715523e-01  1.11628532e+00 -1.51822656e-01]\n",
      "   [-9.46477413e-01 -2.10963264e-01  9.68777716e-01]]\n",
      "\n",
      "  [[ 1.57837355e+00  1.85300782e-01  8.10187340e-01]\n",
      "   [ 1.51580572e+00 -2.78805941e-01 -1.61838102e+00]\n",
      "   [ 1.27503676e-02 -6.05727971e-01 -7.93595463e-02]\n",
      "   ...\n",
      "   [-6.94058776e-01  1.37743342e+00 -1.09587538e+00]\n",
      "   [-1.51339710e+00  5.43470562e-01  2.05749559e+00]\n",
      "   [-4.19087738e-01  6.49064481e-01 -1.68741870e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.67525542e+00 -1.79582268e-01 -4.33079243e-01]\n",
      "   [ 4.09864426e-01 -8.07938457e-01 -2.25169730e+00]\n",
      "   [ 7.38651812e-01  2.04379112e-01  2.16741395e+00]\n",
      "   ...\n",
      "   [-1.30117521e-01  5.61467171e-01  1.10680354e+00]\n",
      "   [ 2.68664777e-01  1.70575941e+00  1.89366370e-01]\n",
      "   [ 1.08010203e-01  1.40325415e+00 -5.66882849e-01]]\n",
      "\n",
      "  [[-1.22021997e+00 -1.12413323e+00  1.87591839e+00]\n",
      "   [-4.93362308e-01 -3.69128913e-01  4.79618043e-01]\n",
      "   [-1.60573506e+00 -5.27263343e-01 -3.04369950e+00]\n",
      "   ...\n",
      "   [ 3.88191283e-01  2.37226352e-01 -4.20297354e-01]\n",
      "   [ 9.67904985e-01  1.81568593e-01  1.76559317e+00]\n",
      "   [-8.85139167e-01 -9.73340631e-01 -3.79292339e-01]]\n",
      "\n",
      "  [[ 1.57265592e+00 -8.56505871e-01 -7.44313717e-01]\n",
      "   [-8.83709669e-01  7.92439282e-01 -7.72657335e-01]\n",
      "   [ 1.31309986e+00  3.58514285e+00 -9.03806150e-01]\n",
      "   ...\n",
      "   [-6.60586298e-01  8.78515482e-01  1.48160756e-02]\n",
      "   [ 3.44149143e-01 -5.04228659e-02 -1.15947127e+00]\n",
      "   [ 1.15077600e-01  5.02453923e-01 -4.08723265e-01]]]], shape=(4, 100, 100, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tensor_h = tf.random.normal([4,100,100,3]) #4 imagenes, de tama;o 100x10 y son de color\n",
    "print(tensor_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 100, 3), dtype=float32, numpy=\n",
       "array([[[-1.0005447 , -0.65210474,  1.0932283 ],\n",
       "        [ 0.9873821 ,  1.3538003 , -1.4561988 ],\n",
       "        [ 0.83116966,  0.22771636,  0.32075414],\n",
       "        ...,\n",
       "        [ 0.74992913, -2.497887  , -1.7881068 ],\n",
       "        [ 0.7640139 , -0.10335645,  0.58183867],\n",
       "        [ 1.2268524 ,  2.4096532 , -0.14627773]],\n",
       "\n",
       "       [[-0.26210663,  0.02946007, -0.79870033],\n",
       "        [ 0.5546421 , -0.9558118 ,  0.66495776],\n",
       "        [-0.43975553, -0.99076253,  1.2067536 ],\n",
       "        ...,\n",
       "        [ 0.12925345,  2.5126889 , -0.7376031 ],\n",
       "        [-1.5651593 , -0.12398954, -1.042342  ],\n",
       "        [-0.7145865 ,  0.57898533, -0.5279085 ]],\n",
       "\n",
       "       [[ 0.5925429 , -0.38032785,  1.3792751 ],\n",
       "        [-0.28443217, -0.11559328, -2.1579041 ],\n",
       "        [ 0.99484515, -0.04684065,  1.4220164 ],\n",
       "        ...,\n",
       "        [ 1.1392823 , -2.4984765 , -0.00977995],\n",
       "        [-1.4125668 ,  1.1749815 , -0.54623836],\n",
       "        [-0.99680907,  0.6519175 , -2.5883255 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.4085562 , -0.78057   , -1.3915948 ],\n",
       "        [ 2.7547832 ,  0.5021127 , -0.2699177 ],\n",
       "        [ 0.09897541,  1.1669062 ,  0.09828345],\n",
       "        ...,\n",
       "        [-0.09743167, -0.20321569, -0.5593938 ],\n",
       "        [-1.1267234 , -0.85388726,  0.70427585],\n",
       "        [-0.6228582 , -1.395633  ,  0.83961976]],\n",
       "\n",
       "       [[-0.28684604,  0.67475235,  1.1021341 ],\n",
       "        [ 0.78621936,  0.43922463, -1.1809787 ],\n",
       "        [-1.0017611 ,  0.5084144 , -0.4001343 ],\n",
       "        ...,\n",
       "        [ 1.7870498 , -0.22339197, -0.8221724 ],\n",
       "        [ 0.04724544, -0.24623942,  2.0340242 ],\n",
       "        [-0.07206627,  0.28392795,  0.64317304]],\n",
       "\n",
       "       [[ 0.2967613 ,  1.1978196 , -1.0743426 ],\n",
       "        [ 0.05053023, -1.6099441 , -1.94615   ],\n",
       "        [ 0.2265255 , -0.7541545 , -2.3158128 ],\n",
       "        ...,\n",
       "        [ 1.7921426 ,  0.328614  ,  0.7782646 ],\n",
       "        [ 0.53045857,  0.3052416 , -2.011184  ],\n",
       "        [ 1.0206969 , -0.77850187, -0.11614455]]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extrayendo solo primera imagen\n",
    "\n",
    "tensor_h[0,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 100, 100, 3), dtype=float32, numpy=\n",
       "array([[[[-1.0005447 , -0.65210474,  1.0932283 ],\n",
       "         [ 0.9873821 ,  1.3538003 , -1.4561988 ],\n",
       "         [ 0.83116966,  0.22771636,  0.32075414],\n",
       "         ...,\n",
       "         [ 0.74992913, -2.497887  , -1.7881068 ],\n",
       "         [ 0.7640139 , -0.10335645,  0.58183867],\n",
       "         [ 1.2268524 ,  2.4096532 , -0.14627773]],\n",
       "\n",
       "        [[-0.26210663,  0.02946007, -0.79870033],\n",
       "         [ 0.5546421 , -0.9558118 ,  0.66495776],\n",
       "         [-0.43975553, -0.99076253,  1.2067536 ],\n",
       "         ...,\n",
       "         [ 0.12925345,  2.5126889 , -0.7376031 ],\n",
       "         [-1.5651593 , -0.12398954, -1.042342  ],\n",
       "         [-0.7145865 ,  0.57898533, -0.5279085 ]],\n",
       "\n",
       "        [[ 0.5925429 , -0.38032785,  1.3792751 ],\n",
       "         [-0.28443217, -0.11559328, -2.1579041 ],\n",
       "         [ 0.99484515, -0.04684065,  1.4220164 ],\n",
       "         ...,\n",
       "         [ 1.1392823 , -2.4984765 , -0.00977995],\n",
       "         [-1.4125668 ,  1.1749815 , -0.54623836],\n",
       "         [-0.99680907,  0.6519175 , -2.5883255 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.4085562 , -0.78057   , -1.3915948 ],\n",
       "         [ 2.7547832 ,  0.5021127 , -0.2699177 ],\n",
       "         [ 0.09897541,  1.1669062 ,  0.09828345],\n",
       "         ...,\n",
       "         [-0.09743167, -0.20321569, -0.5593938 ],\n",
       "         [-1.1267234 , -0.85388726,  0.70427585],\n",
       "         [-0.6228582 , -1.395633  ,  0.83961976]],\n",
       "\n",
       "        [[-0.28684604,  0.67475235,  1.1021341 ],\n",
       "         [ 0.78621936,  0.43922463, -1.1809787 ],\n",
       "         [-1.0017611 ,  0.5084144 , -0.4001343 ],\n",
       "         ...,\n",
       "         [ 1.7870498 , -0.22339197, -0.8221724 ],\n",
       "         [ 0.04724544, -0.24623942,  2.0340242 ],\n",
       "         [-0.07206627,  0.28392795,  0.64317304]],\n",
       "\n",
       "        [[ 0.2967613 ,  1.1978196 , -1.0743426 ],\n",
       "         [ 0.05053023, -1.6099441 , -1.94615   ],\n",
       "         [ 0.2265255 , -0.7541545 , -2.3158128 ],\n",
       "         ...,\n",
       "         [ 1.7921426 ,  0.328614  ,  0.7782646 ],\n",
       "         [ 0.53045857,  0.3052416 , -2.011184  ],\n",
       "         [ 1.0206969 , -0.77850187, -0.11614455]]],\n",
       "\n",
       "\n",
       "       [[[-0.06282248, -1.1737285 ,  1.2696502 ],\n",
       "         [ 0.9939776 ,  0.13431159,  1.164995  ],\n",
       "         [-2.7641988 , -1.005247  ,  1.019165  ],\n",
       "         ...,\n",
       "         [-1.071494  , -0.14211345, -0.86154807],\n",
       "         [ 1.9934032 ,  0.5073948 ,  1.0156586 ],\n",
       "         [ 0.23103867, -1.6355573 , -0.28336713]],\n",
       "\n",
       "        [[ 0.3602591 ,  0.49331242, -1.3566948 ],\n",
       "         [ 0.38575622, -0.435308  ,  1.8476231 ],\n",
       "         [-0.10359722,  0.8017931 ,  1.1061543 ],\n",
       "         ...,\n",
       "         [ 0.4695189 ,  0.82181054, -0.8896512 ],\n",
       "         [ 1.1103548 ,  1.7626435 , -0.563014  ],\n",
       "         [ 0.48929173, -0.508557  , -1.1220131 ]],\n",
       "\n",
       "        [[ 2.254359  , -0.7477469 , -0.22342448],\n",
       "         [-0.95299304, -0.09454349,  0.9445237 ],\n",
       "         [ 0.05642834,  0.4392946 ,  0.90746737],\n",
       "         ...,\n",
       "         [-1.3361384 , -0.9721597 ,  0.990367  ],\n",
       "         [ 0.35585195,  1.6811861 ,  1.6275316 ],\n",
       "         [ 2.2246003 , -0.9752583 ,  0.24200375]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4428945 ,  2.093323  , -0.59941596],\n",
       "         [-1.0109296 , -0.5321319 ,  0.80733293],\n",
       "         [-0.18246992, -0.56421864, -0.18618715],\n",
       "         ...,\n",
       "         [-0.3852658 , -0.34962344, -0.46902215],\n",
       "         [-0.06226117, -0.7371377 , -0.93429583],\n",
       "         [-0.0378808 ,  0.13204746,  0.8602219 ]],\n",
       "\n",
       "        [[-0.33345613, -1.8363318 ,  0.3842996 ],\n",
       "         [-1.066011  ,  1.4979701 , -0.49946246],\n",
       "         [-0.00298765, -0.32528543, -0.8633321 ],\n",
       "         ...,\n",
       "         [-0.30698046, -0.34555304, -0.00859103],\n",
       "         [ 0.6368762 ,  0.35224214, -0.05934213],\n",
       "         [-1.4657404 , -0.5889124 ,  0.8911867 ]],\n",
       "\n",
       "        [[-1.4813777 , -0.09041369,  0.02822371],\n",
       "         [-0.63117707,  0.18199353, -0.350718  ],\n",
       "         [-1.584361  , -0.47979712,  0.7695264 ],\n",
       "         ...,\n",
       "         [-1.0594356 , -0.04611198,  0.7215539 ],\n",
       "         [-0.5192663 , -2.01746   ,  0.12445329],\n",
       "         [-1.5395068 ,  1.0330166 ,  0.50933564]]]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_h[::2,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 100, 100, 3), dtype=float32, numpy=\n",
       "array([[[[-1.11900032e+00,  4.81359005e-01, -9.91551816e-01],\n",
       "         [-1.63031006e+00,  1.72501713e-01,  8.15835357e-01],\n",
       "         [ 1.59688011e-01,  1.33077705e+00,  4.56041634e-01],\n",
       "         ...,\n",
       "         [ 1.91381383e+00, -1.89852047e+00, -7.55266726e-01],\n",
       "         [-2.84153819e-01,  1.40374526e-02,  4.47298065e-02],\n",
       "         [ 6.09242141e-01,  7.61439949e-02, -2.10788584e+00]],\n",
       "\n",
       "        [[ 7.35600114e-01,  4.78352547e-01,  8.96837592e-01],\n",
       "         [ 6.47120416e-01, -1.09583867e+00, -1.19604361e+00],\n",
       "         [ 6.66618526e-01,  3.98706526e-01,  1.80406556e-01],\n",
       "         ...,\n",
       "         [ 9.84750032e-01, -1.97311834e-01, -3.01506847e-01],\n",
       "         [ 3.38715523e-01,  1.11628532e+00, -1.51822656e-01],\n",
       "         [-9.46477413e-01, -2.10963264e-01,  9.68777716e-01]],\n",
       "\n",
       "        [[ 1.57837355e+00,  1.85300782e-01,  8.10187340e-01],\n",
       "         [ 1.51580572e+00, -2.78805941e-01, -1.61838102e+00],\n",
       "         [ 1.27503676e-02, -6.05727971e-01, -7.93595463e-02],\n",
       "         ...,\n",
       "         [-6.94058776e-01,  1.37743342e+00, -1.09587538e+00],\n",
       "         [-1.51339710e+00,  5.43470562e-01,  2.05749559e+00],\n",
       "         [-4.19087738e-01,  6.49064481e-01, -1.68741870e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.67525542e+00, -1.79582268e-01, -4.33079243e-01],\n",
       "         [ 4.09864426e-01, -8.07938457e-01, -2.25169730e+00],\n",
       "         [ 7.38651812e-01,  2.04379112e-01,  2.16741395e+00],\n",
       "         ...,\n",
       "         [-1.30117521e-01,  5.61467171e-01,  1.10680354e+00],\n",
       "         [ 2.68664777e-01,  1.70575941e+00,  1.89366370e-01],\n",
       "         [ 1.08010203e-01,  1.40325415e+00, -5.66882849e-01]],\n",
       "\n",
       "        [[-1.22021997e+00, -1.12413323e+00,  1.87591839e+00],\n",
       "         [-4.93362308e-01, -3.69128913e-01,  4.79618043e-01],\n",
       "         [-1.60573506e+00, -5.27263343e-01, -3.04369950e+00],\n",
       "         ...,\n",
       "         [ 3.88191283e-01,  2.37226352e-01, -4.20297354e-01],\n",
       "         [ 9.67904985e-01,  1.81568593e-01,  1.76559317e+00],\n",
       "         [-8.85139167e-01, -9.73340631e-01, -3.79292339e-01]],\n",
       "\n",
       "        [[ 1.57265592e+00, -8.56505871e-01, -7.44313717e-01],\n",
       "         [-8.83709669e-01,  7.92439282e-01, -7.72657335e-01],\n",
       "         [ 1.31309986e+00,  3.58514285e+00, -9.03806150e-01],\n",
       "         ...,\n",
       "         [-6.60586298e-01,  8.78515482e-01,  1.48160756e-02],\n",
       "         [ 3.44149143e-01, -5.04228659e-02, -1.15947127e+00],\n",
       "         [ 1.15077600e-01,  5.02453923e-01, -4.08723265e-01]]],\n",
       "\n",
       "\n",
       "       [[[-6.28224835e-02, -1.17372847e+00,  1.26965022e+00],\n",
       "         [ 9.93977606e-01,  1.34311587e-01,  1.16499496e+00],\n",
       "         [-2.76419878e+00, -1.00524700e+00,  1.01916504e+00],\n",
       "         ...,\n",
       "         [-1.07149398e+00, -1.42113447e-01, -8.61548066e-01],\n",
       "         [ 1.99340320e+00,  5.07394791e-01,  1.01565862e+00],\n",
       "         [ 2.31038675e-01, -1.63555729e+00, -2.83367127e-01]],\n",
       "\n",
       "        [[ 3.60259086e-01,  4.93312418e-01, -1.35669482e+00],\n",
       "         [ 3.85756224e-01, -4.35308009e-01,  1.84762311e+00],\n",
       "         [-1.03597224e-01,  8.01793098e-01,  1.10615432e+00],\n",
       "         ...,\n",
       "         [ 4.69518900e-01,  8.21810544e-01, -8.89651179e-01],\n",
       "         [ 1.11035478e+00,  1.76264346e+00, -5.63013971e-01],\n",
       "         [ 4.89291728e-01, -5.08557022e-01, -1.12201309e+00]],\n",
       "\n",
       "        [[ 2.25435901e+00, -7.47746885e-01, -2.23424479e-01],\n",
       "         [-9.52993035e-01, -9.45434943e-02,  9.44523692e-01],\n",
       "         [ 5.64283431e-02,  4.39294606e-01,  9.07467365e-01],\n",
       "         ...,\n",
       "         [-1.33613837e+00, -9.72159684e-01,  9.90366995e-01],\n",
       "         [ 3.55851948e-01,  1.68118608e+00,  1.62753165e+00],\n",
       "         [ 2.22460032e+00, -9.75258291e-01,  2.42003754e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 4.42894489e-01,  2.09332299e+00, -5.99415958e-01],\n",
       "         [-1.01092958e+00, -5.32131910e-01,  8.07332933e-01],\n",
       "         [-1.82469919e-01, -5.64218640e-01, -1.86187148e-01],\n",
       "         ...,\n",
       "         [-3.85265797e-01, -3.49623442e-01, -4.69022155e-01],\n",
       "         [-6.22611679e-02, -7.37137675e-01, -9.34295833e-01],\n",
       "         [-3.78808007e-02,  1.32047459e-01,  8.60221922e-01]],\n",
       "\n",
       "        [[-3.33456129e-01, -1.83633184e+00,  3.84299606e-01],\n",
       "         [-1.06601095e+00,  1.49797010e+00, -4.99462456e-01],\n",
       "         [-2.98765115e-03, -3.25285435e-01, -8.63332093e-01],\n",
       "         ...,\n",
       "         [-3.06980461e-01, -3.45553041e-01, -8.59102793e-03],\n",
       "         [ 6.36876225e-01,  3.52242142e-01, -5.93421347e-02],\n",
       "         [-1.46574044e+00, -5.88912427e-01,  8.91186714e-01]],\n",
       "\n",
       "        [[-1.48137772e+00, -9.04136896e-02,  2.82237139e-02],\n",
       "         [-6.31177068e-01,  1.81993529e-01, -3.50717992e-01],\n",
       "         [-1.58436096e+00, -4.79797125e-01,  7.69526422e-01],\n",
       "         ...,\n",
       "         [-1.05943561e+00, -4.61119823e-02,  7.21553922e-01],\n",
       "         [-5.19266307e-01, -2.01746011e+00,  1.24453291e-01],\n",
       "         [-1.53950679e+00,  1.03301656e+00,  5.09335637e-01]]],\n",
       "\n",
       "\n",
       "       [[[-1.50014043e-01, -1.39248478e+00, -6.77798092e-01],\n",
       "         [-1.98917127e+00,  1.06064096e-01, -1.30953580e-01],\n",
       "         [ 8.82990479e-01, -3.51989865e-02, -2.73631549e+00],\n",
       "         ...,\n",
       "         [ 1.68346196e-01, -1.36832282e-01,  1.14109230e+00],\n",
       "         [-6.58138096e-01, -3.51297902e-03, -8.36209208e-02],\n",
       "         [-2.38436174e+00, -6.61761165e-01, -9.58511755e-02]],\n",
       "\n",
       "        [[ 1.48184288e+00,  2.08133146e-01,  4.21317220e-01],\n",
       "         [ 1.01424563e+00,  1.42577159e+00,  7.09047318e-01],\n",
       "         [ 1.51224232e+00, -1.06013668e+00,  1.38933808e-01],\n",
       "         ...,\n",
       "         [-6.02511823e-01, -5.42413533e-01,  2.38507628e+00],\n",
       "         [-1.34048775e-01,  4.48654979e-01,  1.62817419e-01],\n",
       "         [-4.45997685e-01,  4.96715039e-01,  2.44963431e+00]],\n",
       "\n",
       "        [[-5.28492093e-01,  1.01954412e+00,  3.71956617e-01],\n",
       "         [-2.35002220e-01, -2.26174027e-01,  6.46820426e-01],\n",
       "         [ 5.04460871e-01,  4.66103554e-01,  5.23691952e-01],\n",
       "         ...,\n",
       "         [ 1.32431507e-01, -1.61450422e+00,  1.15370817e-01],\n",
       "         [-1.48582697e+00,  5.71251214e-01,  9.95844901e-01],\n",
       "         [-5.16170859e-02, -2.62882739e-01, -7.34930396e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-9.73277152e-01,  1.76439905e+00,  6.18197136e-02],\n",
       "         [-1.67513585e+00, -1.04321487e-01,  1.68196484e-01],\n",
       "         [-6.17433265e-02, -1.28547415e-01,  9.15369153e-01],\n",
       "         ...,\n",
       "         [-1.92184007e+00, -2.54262425e-02,  1.25219202e+00],\n",
       "         [-1.71559358e+00, -1.39991176e+00,  9.04901147e-01],\n",
       "         [-6.18344903e-01,  7.78819442e-01,  4.18251812e-01]],\n",
       "\n",
       "        [[-8.28820989e-02, -3.67210537e-01,  1.98557007e+00],\n",
       "         [ 5.10159135e-01, -2.51047671e-01, -1.67831337e+00],\n",
       "         [ 4.08002794e-01, -1.23062801e+00,  6.86083198e-01],\n",
       "         ...,\n",
       "         [ 4.99584556e-01, -2.71540880e-01, -5.55180430e-01],\n",
       "         [ 1.65365249e-01, -1.10307872e+00,  2.71765041e+00],\n",
       "         [ 4.35609549e-01, -5.90904772e-01,  1.29071903e+00]],\n",
       "\n",
       "        [[ 1.42517710e+00, -1.60294187e+00, -9.68891740e-01],\n",
       "         [-2.96124160e-01,  1.48607743e+00, -1.16679549e+00],\n",
       "         [-1.14158320e+00,  1.52382994e+00,  5.82725763e-01],\n",
       "         ...,\n",
       "         [-7.41295338e-01,  2.81678110e-01,  6.39159560e-01],\n",
       "         [ 9.36445177e-01, -9.69879150e-01,  1.47812927e+00],\n",
       "         [-1.15554726e+00,  9.91636336e-01, -7.70962238e-02]]],\n",
       "\n",
       "\n",
       "       [[[-1.00054467e+00, -6.52104735e-01,  1.09322834e+00],\n",
       "         [ 9.87382114e-01,  1.35380030e+00, -1.45619881e+00],\n",
       "         [ 8.31169665e-01,  2.27716357e-01,  3.20754141e-01],\n",
       "         ...,\n",
       "         [ 7.49929130e-01, -2.49788690e+00, -1.78810680e+00],\n",
       "         [ 7.64013886e-01, -1.03356451e-01,  5.81838667e-01],\n",
       "         [ 1.22685242e+00,  2.40965319e+00, -1.46277726e-01]],\n",
       "\n",
       "        [[-2.62106627e-01,  2.94600725e-02, -7.98700333e-01],\n",
       "         [ 5.54642081e-01, -9.55811799e-01,  6.64957762e-01],\n",
       "         [-4.39755529e-01, -9.90762532e-01,  1.20675361e+00],\n",
       "         ...,\n",
       "         [ 1.29253447e-01,  2.51268888e+00, -7.37603128e-01],\n",
       "         [-1.56515932e+00, -1.23989537e-01, -1.04234195e+00],\n",
       "         [-7.14586496e-01,  5.78985333e-01, -5.27908504e-01]],\n",
       "\n",
       "        [[ 5.92542887e-01, -3.80327851e-01,  1.37927508e+00],\n",
       "         [-2.84432173e-01, -1.15593277e-01, -2.15790415e+00],\n",
       "         [ 9.94845152e-01, -4.68406528e-02,  1.42201638e+00],\n",
       "         ...,\n",
       "         [ 1.13928235e+00, -2.49847651e+00, -9.77994781e-03],\n",
       "         [-1.41256678e+00,  1.17498147e+00, -5.46238363e-01],\n",
       "         [-9.96809065e-01,  6.51917517e-01, -2.58832550e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.40855622e+00, -7.80569971e-01, -1.39159477e+00],\n",
       "         [ 2.75478315e+00,  5.02112687e-01, -2.69917697e-01],\n",
       "         [ 9.89754051e-02,  1.16690624e+00,  9.82834548e-02],\n",
       "         ...,\n",
       "         [-9.74316671e-02, -2.03215688e-01, -5.59393823e-01],\n",
       "         [-1.12672341e+00, -8.53887260e-01,  7.04275846e-01],\n",
       "         [-6.22858226e-01, -1.39563298e+00,  8.39619756e-01]],\n",
       "\n",
       "        [[-2.86846042e-01,  6.74752355e-01,  1.10213411e+00],\n",
       "         [ 7.86219358e-01,  4.39224631e-01, -1.18097866e+00],\n",
       "         [-1.00176108e+00,  5.08414388e-01, -4.00134295e-01],\n",
       "         ...,\n",
       "         [ 1.78704977e+00, -2.23391965e-01, -8.22172403e-01],\n",
       "         [ 4.72454391e-02, -2.46239424e-01,  2.03402424e+00],\n",
       "         [-7.20662698e-02,  2.83927947e-01,  6.43173039e-01]],\n",
       "\n",
       "        [[ 2.96761304e-01,  1.19781959e+00, -1.07434261e+00],\n",
       "         [ 5.05302288e-02, -1.60994411e+00, -1.94614995e+00],\n",
       "         [ 2.26525500e-01, -7.54154503e-01, -2.31581283e+00],\n",
       "         ...,\n",
       "         [ 1.79214263e+00,  3.28613997e-01,  7.78264582e-01],\n",
       "         [ 5.30458570e-01,  3.05241615e-01, -2.01118398e+00],\n",
       "         [ 1.02069688e+00, -7.78501868e-01, -1.16144545e-01]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Segmantando data del último elemento\n",
    "\n",
    "tensor_h[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexación\n",
    "\n",
    "Formato básico es tensor[d1][d2][d3]......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.8111355>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_h[0][19][39][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(tf.gather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5546421, 0.5044609], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [[0,1,1,0],[1,2,2,0]]\n",
    "tf.gather_nd(tensor_h,indices=indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modificación de dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "const_d_1 = tf.constant([[1,2,3,4]],shape=[2,2])\n",
    "print(const_d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2, 2)\n",
      "tf.Tensor([2 2], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#Maneras para devolver la dim\n",
    "\n",
    "print(const_d_1.shape)\n",
    "print(const_d_1.get_shape())\n",
    "print(tf.shape(const_d_1)) #Ouput de este, es un tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reshape in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "reshape(tensor, shape, name=None)\n",
      "    Reshapes a tensor.\n",
      "    \n",
      "    Given `tensor`, this operation returns a new `tf.Tensor` that has the same\n",
      "    values as `tensor` in the same order, except with a new shape given by\n",
      "    `shape`.\n",
      "    \n",
      "    >>> t1 = [[1, 2, 3],\n",
      "    ...       [4, 5, 6]]\n",
      "    >>> print(tf.shape(t1).numpy())\n",
      "    [2 3]\n",
      "    >>> t2 = tf.reshape(t1, [6])\n",
      "    >>> t2\n",
      "    <tf.Tensor: shape=(6,), dtype=int32,\n",
      "      numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n",
      "    >>> tf.reshape(t2, [3, 2])\n",
      "    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "      array([[1, 2],\n",
      "             [3, 4],\n",
      "             [5, 6]], dtype=int32)>\n",
      "    \n",
      "    The `tf.reshape` does not change the order of or the total number of elements\n",
      "    in the tensor, and so it can reuse the underlying data buffer. This makes it\n",
      "    a fast operation independent of how big of a tensor it is operating on.\n",
      "    \n",
      "    >>> tf.reshape([1, 2, 3], [2, 2])\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    InvalidArgumentError: Input to reshape is a tensor with 3 values, but the\n",
      "    requested shape has 4\n",
      "    \n",
      "    To instead reorder the data to rearrange the dimensions of a tensor, see\n",
      "    `tf.transpose`.\n",
      "    \n",
      "    >>> t = [[1, 2, 3],\n",
      "    ...      [4, 5, 6]]\n",
      "    >>> tf.reshape(t, [3, 2]).numpy()\n",
      "    array([[1, 2],\n",
      "           [3, 4],\n",
      "           [5, 6]], dtype=int32)\n",
      "    >>> tf.transpose(t, perm=[1, 0]).numpy()\n",
      "    array([[1, 4],\n",
      "           [2, 5],\n",
      "           [3, 6]], dtype=int32)\n",
      "    \n",
      "    If one component of `shape` is the special value -1, the size of that\n",
      "    dimension is computed so that the total size remains constant.  In particular,\n",
      "    a `shape` of `[-1]` flattens into 1-D.  At most one component of `shape` can\n",
      "    be -1.\n",
      "    \n",
      "    >>> t = [[1, 2, 3],\n",
      "    ...      [4, 5, 6]]\n",
      "    >>> tf.reshape(t, [-1])\n",
      "    <tf.Tensor: shape=(6,), dtype=int32,\n",
      "      numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n",
      "    >>> tf.reshape(t, [3, -1])\n",
      "    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "      array([[1, 2],\n",
      "             [3, 4],\n",
      "             [5, 6]], dtype=int32)>\n",
      "    >>> tf.reshape(t, [-1, 2])\n",
      "    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "      array([[1, 2],\n",
      "             [3, 4],\n",
      "             [5, 6]], dtype=int32)>\n",
      "    \n",
      "    `tf.reshape(t, [])` reshapes a tensor `t` with one element to a scalar.\n",
      "    \n",
      "    >>> tf.reshape([7], []).numpy()\n",
      "    7\n",
      "    \n",
      "    More examples:\n",
      "    \n",
      "    >>> t = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    >>> print(tf.shape(t).numpy())\n",
      "    [9]\n",
      "    >>> tf.reshape(t, [3, 3])\n",
      "    <tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
      "      array([[1, 2, 3],\n",
      "             [4, 5, 6],\n",
      "             [7, 8, 9]], dtype=int32)>\n",
      "    \n",
      "    >>> t = [[[1, 1], [2, 2]],\n",
      "    ...      [[3, 3], [4, 4]]]\n",
      "    >>> print(tf.shape(t).numpy())\n",
      "    [2 2 2]\n",
      "    >>> tf.reshape(t, [2, 4])\n",
      "    <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n",
      "      array([[1, 1, 2, 2],\n",
      "             [3, 3, 4, 4]], dtype=int32)>\n",
      "    \n",
      "    >>> t = [[[1, 1, 1],\n",
      "    ...       [2, 2, 2]],\n",
      "    ...      [[3, 3, 3],\n",
      "    ...       [4, 4, 4]],\n",
      "    ...      [[5, 5, 5],\n",
      "    ...       [6, 6, 6]]]\n",
      "    >>> print(tf.shape(t).numpy())\n",
      "    [3 2 3]\n",
      "    >>> # Pass '[-1]' to flatten 't'.\n",
      "    >>> tf.reshape(t, [-1])\n",
      "    <tf.Tensor: shape=(18,), dtype=int32,\n",
      "      numpy=array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6],\n",
      "      dtype=int32)>\n",
      "    >>> # -- Using -1 to infer the shape --\n",
      "    >>> # Here -1 is inferred to be 9:\n",
      "    >>> tf.reshape(t, [2, -1])\n",
      "    <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
      "      array([[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "             [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)>\n",
      "    >>> # -1 is inferred to be 2:\n",
      "    >>> tf.reshape(t, [-1, 9])\n",
      "    <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
      "      array([[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "             [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)>\n",
      "    >>> # -1 is inferred to be 3:\n",
      "    >>> tf.reshape(t, [ 2, -1, 3])\n",
      "    <tf.Tensor: shape=(2, 3, 3), dtype=int32, numpy=\n",
      "      array([[[1, 1, 1],\n",
      "              [2, 2, 2],\n",
      "              [3, 3, 3]],\n",
      "             [[4, 4, 4],\n",
      "              [5, 5, 5],\n",
      "              [6, 6, 6]]], dtype=int32)>\n",
      "    \n",
      "    Args:\n",
      "      tensor: A `Tensor`.\n",
      "      shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        Defines the shape of the output tensor.\n",
      "      name: Optional string. A name for the operation.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "resphape_1 = tf.constant([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resphape_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resphape_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(resphape_1,(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function expand_dims_v2 in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "expand_dims_v2(input, axis, name=None)\n",
      "    Returns a tensor with a length 1 axis inserted at index `axis`.\n",
      "    \n",
      "    Given a tensor `input`, this operation inserts a dimension of length 1 at the\n",
      "    dimension index `axis` of `input`'s shape. The dimension index follows Python\n",
      "    indexing rules: It's zero-based, a negative index it is counted backward\n",
      "    from the end.\n",
      "    \n",
      "    This operation is useful to:\n",
      "    \n",
      "    * Add an outer \"batch\" dimension to a single element.\n",
      "    * Align axes for broadcasting.\n",
      "    * To add an inner vector length axis to a tensor of scalars.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    If you have a single image of shape `[height, width, channels]`:\n",
      "    \n",
      "    >>> image = tf.zeros([10,10,3])\n",
      "    \n",
      "    You can add an outer `batch` axis by passing `axis=0`:\n",
      "    \n",
      "    >>> tf.expand_dims(image, axis=0).shape.as_list()\n",
      "    [1, 10, 10, 3]\n",
      "    \n",
      "    The new axis location matches Python `list.insert(axis, 1)`:\n",
      "    \n",
      "    >>> tf.expand_dims(image, axis=1).shape.as_list()\n",
      "    [10, 1, 10, 3]\n",
      "    \n",
      "    Following standard Python indexing rules, a negative `axis` counts from the\n",
      "    end so `axis=-1` adds an inner most dimension:\n",
      "    \n",
      "    >>> tf.expand_dims(image, -1).shape.as_list()\n",
      "    [10, 10, 3, 1]\n",
      "    \n",
      "    This operation requires that `axis` is a valid index for `input.shape`,\n",
      "    following Python indexing rules:\n",
      "    \n",
      "    ```\n",
      "    -1-tf.rank(input) <= axis <= tf.rank(input)\n",
      "    ```\n",
      "    \n",
      "    This operation is related to:\n",
      "    \n",
      "    * `tf.squeeze`, which removes dimensions of size 1.\n",
      "    * `tf.reshape`, which provides more flexible reshaping capability.\n",
      "    * `tf.sparse.expand_dims`, which provides this functionality for\n",
      "      `tf.SparseTensor`\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`.\n",
      "      axis: Integer specifying the dimension index at which to expand the\n",
      "        shape of `input`. Given an input of D dimensions, `axis` must be in range\n",
      "        `[-(D+1), D]` (inclusive).\n",
      "      name: Optional string. The name of the output `Tensor`.\n",
      "    \n",
      "    Returns:\n",
      "      A tensor with the same data as `input`, with an additional dimension\n",
      "      inserted at the index specified by `axis`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `axis` is not specified.\n",
      "      InvalidArgumentError: If `axis` is out of range `[-(D+1), D]`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.expand_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones originales (100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "expand_sample_1 = tf.random.normal([100,100,3])\n",
    "print('Dimensiones originales',expand_sample_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregando una dimension, antes de la primera dimension, (axis=0) (1, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Agregando una dimension, antes de la primera dimension, (axis=0)', tf.expand_dims(expand_sample_1,axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregando una dimension, antes de la segunda dimension, (axis=1) (100, 1, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Agregando una dimension, antes de la segunda dimension, (axis=1)', tf.expand_dims(expand_sample_1,axis=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregando dimension, después de la ULTIMA dimensión (axis=-1) (100, 100, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Agregando dimension, después de la ULTIMA dimensión (axis=-1)',tf.expand_dims(expand_sample_1,axis=-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squeezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function squeeze_v2 in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "squeeze_v2(input, axis=None, name=None)\n",
      "    Removes dimensions of size 1 from the shape of a tensor.\n",
      "    \n",
      "    Given a tensor `input`, this operation returns a tensor of the same type with\n",
      "    all dimensions of size 1 removed. If you don't want to remove all size 1\n",
      "    dimensions, you can remove specific size 1 dimensions by specifying\n",
      "    `axis`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\n",
      "    tf.shape(tf.squeeze(t))  # [2, 3]\n",
      "    ```\n",
      "    \n",
      "    Or, to remove specific size 1 dimensions:\n",
      "    \n",
      "    ```python\n",
      "    # 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\n",
      "    tf.shape(tf.squeeze(t, [2, 4]))  # [1, 2, 3, 1]\n",
      "    ```\n",
      "    \n",
      "    Unlike the older op `tf.compat.v1.squeeze`, this op does not accept a\n",
      "    deprecated `squeeze_dims` argument.\n",
      "    \n",
      "    Note: if `input` is a `tf.RaggedTensor`, then this operation takes `O(N)`\n",
      "    time, where `N` is the number of elements in the squeezed dimensions.\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. The `input` to squeeze.\n",
      "      axis: An optional list of `ints`. Defaults to `[]`. If specified, only\n",
      "        squeezes the dimensions listed. The dimension index starts at 0. It is an\n",
      "        error to squeeze a dimension that is not 1. Must be in the range\n",
      "        `[-rank(input), rank(input))`. Must be specified if `input` is a\n",
      "        `RaggedTensor`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "      Contains the same data as `input`, but has one or more dimensions of\n",
      "      size 1 removed.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: The input cannot be converted to a tensor, or the specified\n",
      "        axis cannot be squeezed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.squeeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "squeeze_sample_1 = tf.random.normal([1,100,100,3])\n",
    "print(squeeze_sample_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "squeezed_sample_1 = tf.squeeze(squeeze_sample_1,axis=0)\n",
    "print(squeezed_sample_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transponer\n",
    "\n",
    "Se ocupa perm cuando hay varias dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function transpose_v2 in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "transpose_v2(a, perm=None, conjugate=False, name='transpose')\n",
      "    Transposes `a`, where `a` is a Tensor.\n",
      "    \n",
      "    Permutes the dimensions according to the value of `perm`.\n",
      "    \n",
      "    The returned tensor's dimension `i` will correspond to the input dimension\n",
      "    `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\n",
      "    of the input tensor. Hence by default, this operation performs a regular\n",
      "    matrix transpose on 2-D input Tensors.\n",
      "    \n",
      "    If conjugate is `True` and `a.dtype` is either `complex64` or `complex128`\n",
      "    then the values of `a` are conjugated and transposed.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    In `numpy` transposes are memory-efficient constant time operations as they\n",
      "    simply return a new view of the same data with adjusted `strides`.\n",
      "    \n",
      "    TensorFlow does not support strides, so `transpose` returns a new tensor with\n",
      "    the items permuted.\n",
      "    @end_compatibility\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
      "    >>> tf.transpose(x)\n",
      "    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "    array([[1, 4],\n",
      "           [2, 5],\n",
      "           [3, 6]], dtype=int32)>\n",
      "    \n",
      "    Equivalently, you could call `tf.transpose(x, perm=[1, 0])`.\n",
      "    \n",
      "    If `x` is complex, setting conjugate=True gives the conjugate transpose:\n",
      "    \n",
      "    >>> x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],\n",
      "    ...                  [4 + 4j, 5 + 5j, 6 + 6j]])\n",
      "    >>> tf.transpose(x, conjugate=True)\n",
      "    <tf.Tensor: shape=(3, 2), dtype=complex128, numpy=\n",
      "    array([[1.-1.j, 4.-4.j],\n",
      "           [2.-2.j, 5.-5.j],\n",
      "           [3.-3.j, 6.-6.j]])>\n",
      "    \n",
      "    'perm' is more useful for n-dimensional tensors where n > 2:\n",
      "    \n",
      "    >>> x = tf.constant([[[ 1,  2,  3],\n",
      "    ...                   [ 4,  5,  6]],\n",
      "    ...                  [[ 7,  8,  9],\n",
      "    ...                   [10, 11, 12]]])\n",
      "    \n",
      "    As above, simply calling `tf.transpose` will default to `perm=[2,1,0]`.\n",
      "    \n",
      "    To take the transpose of the matrices in dimension-0 (such as when you are\n",
      "    transposing matrices where 0 is the batch dimesnion), you would set\n",
      "    `perm=[0,2,1]`.\n",
      "    \n",
      "    >>> tf.transpose(x, perm=[0, 2, 1])\n",
      "    <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      "    array([[[ 1,  4],\n",
      "            [ 2,  5],\n",
      "            [ 3,  6]],\n",
      "            [[ 7, 10],\n",
      "            [ 8, 11],\n",
      "            [ 9, 12]]], dtype=int32)>\n",
      "    \n",
      "    Note: This has a shorthand `linalg.matrix_transpose`):\n",
      "    \n",
      "    Args:\n",
      "      a: A `Tensor`.\n",
      "      perm: A permutation of the dimensions of `a`.  This should be a vector.\n",
      "      conjugate: Optional bool. Setting it to `True` is mathematically equivalent\n",
      "        to tf.math.conj(tf.transpose(input)).\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A transposed `Tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimn originales (2, 3)\n"
     ]
    }
   ],
   "source": [
    "trans_sample_1 = tf.constant([1,2,3,4,5,6],shape=[2,3])\n",
    "print('Dimn originales',trans_sample_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims transpuesta (3, 2)\n"
     ]
    }
   ],
   "source": [
    "transposed_sample_1 = tf.transpose(trans_sample_1)\n",
    "print('Dims transpuesta',transposed_sample_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimn originales (4, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "trans_sample_2 = tf.random.normal([4,100,100,3])\n",
    "print('Dimn originales',trans_sample_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimn luego de transpuesta (4, 100, 3, 100)\n"
     ]
    }
   ],
   "source": [
    "transposed_sample_2 = tf.transpose(trans_sample_2,[0,2,3,1])\n",
    "print('Dimn luego de transpuesta',transposed_sample_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function broadcast_to in module tensorflow.python.ops.gen_array_ops:\n",
      "\n",
      "broadcast_to(input, shape, name=None)\n",
      "    Broadcast an array for a compatible shape.\n",
      "    \n",
      "    Broadcasting is the process of making arrays to have compatible shapes\n",
      "    for arithmetic operations. Two shapes are compatible if for each\n",
      "    dimension pair they are either equal or one of them is one. When trying\n",
      "    to broadcast a Tensor to a shape, it starts with the trailing dimensions,\n",
      "    and works its way forward.\n",
      "    \n",
      "    For example,\n",
      "    \n",
      "    >>> x = tf.constant([1, 2, 3])\n",
      "    >>> y = tf.broadcast_to(x, [3, 3])\n",
      "    >>> print(y)\n",
      "    tf.Tensor(\n",
      "        [[1 2 3]\n",
      "         [1 2 3]\n",
      "         [1 2 3]], shape=(3, 3), dtype=int32)\n",
      "    \n",
      "    In the above example, the input Tensor with the shape of `[1, 3]`\n",
      "    is broadcasted to output Tensor with shape of `[3, 3]`.\n",
      "    \n",
      "    When doing broadcasted operations such as multiplying a tensor\n",
      "    by a scalar, broadcasting (usually) confers some time or space\n",
      "    benefit, as the broadcasted tensor is never materialized.\n",
      "    \n",
      "    However, `broadcast_to` does not carry with it any such benefits.\n",
      "    The newly-created tensor takes the full memory of the broadcasted\n",
      "    shape. (In a graph context, `broadcast_to` might be fused to\n",
      "    subsequent operation and then be optimized away, however.)\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. A Tensor to broadcast.\n",
      "      shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        An 1-D `int` Tensor. The shape of the desired output.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.broadcast_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim original (6,)\n"
     ]
    }
   ],
   "source": [
    "broadcast_sample_1 = tf.constant([1,2,3,4,5,6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim broadcasted (4, 6)\n",
      "[[1 2 3 4 5 6]\n",
      " [1 2 3 4 5 6]\n",
      " [1 2 3 4 5 6]\n",
      " [1 2 3 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "broadcasted_sample_1 = tf.broadcast_to(broadcast_sample_1,shape=[4,6])\n",
    "print('Dim broadcasted', broadcasted_sample_1.shape)\n",
    "print(broadcasted_sample_1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[0,0,0],\n",
    "                [10,10,10],\n",
    "                [20,20,20],\n",
    "                [30,30,30]])\n",
    "\n",
    "b = tf.constant([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1  2  3]\n",
      " [11 12 13]\n",
      " [21 22 23]\n",
      " [31 32 33]], shape=(4, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones artimeticas\n",
    "\n",
    "* Suma\n",
    "* Resta\n",
    "* Multuplicación\n",
    "* Logartimo\n",
    "* Exponente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 4 11]\n",
      " [ 6 17]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[3,5],[4,8]])\n",
    "b = tf.constant([[1,6],[2,9]])\n",
    "\n",
    "\n",
    "print(tf.add(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[13 63]\n",
      " [20 96]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.matmul(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function matmul in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "matmul(a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False, a_is_sparse=False, b_is_sparse=False, name=None)\n",
      "    Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      "    \n",
      "    The inputs must, following any transpositions, be tensors of rank >= 2\n",
      "    where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
      "    and any further outer dimensions specify matching batch size.\n",
      "    \n",
      "    Both matrices must be of the same type. The supported types are:\n",
      "    `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      "    \n",
      "    Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      "    the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      "    by default.\n",
      "    \n",
      "    If one or both of the matrices contain a lot of zeros, a more efficient\n",
      "    multiplication algorithm can be used by setting the corresponding\n",
      "    `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      "    This optimization is only available for plain matrices (rank-2 tensors) with\n",
      "    datatypes `bfloat16` or `float32`.\n",
      "    \n",
      "    A simple 2-D tensor matrix multiplication:\n",
      "    \n",
      "    >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      "    >>> a  # 2-D tensor\n",
      "    <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      "    array([[1, 2, 3],\n",
      "           [4, 5, 6]], dtype=int32)>\n",
      "    >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      "    >>> b  # 2-D tensor\n",
      "    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "    array([[ 7,  8],\n",
      "           [ 9, 10],\n",
      "           [11, 12]], dtype=int32)>\n",
      "    >>> c = tf.matmul(a, b)\n",
      "    >>> c  # `a` * `b`\n",
      "    <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
      "    array([[ 58,  64],\n",
      "           [139, 154]], dtype=int32)>\n",
      "    \n",
      "    A batch matrix multiplication with batch shape [2]:\n",
      "    \n",
      "    >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
      "    >>> a  # 3-D tensor\n",
      "    <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
      "    array([[[ 1,  2,  3],\n",
      "            [ 4,  5,  6]],\n",
      "           [[ 7,  8,  9],\n",
      "            [10, 11, 12]]], dtype=int32)>\n",
      "    >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
      "    >>> b  # 3-D tensor\n",
      "    <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
      "    array([[[13, 14],\n",
      "            [15, 16],\n",
      "            [17, 18]],\n",
      "           [[19, 20],\n",
      "            [21, 22],\n",
      "            [23, 24]]], dtype=int32)>\n",
      "    >>> c = tf.matmul(a, b)\n",
      "    >>> c  # `a` * `b`\n",
      "    <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
      "    array([[[ 94, 100],\n",
      "            [229, 244]],\n",
      "           [[508, 532],\n",
      "            [697, 730]]], dtype=int32)>\n",
      "    \n",
      "    Since python >= 3.5 the @ operator is supported\n",
      "    (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
      "    it simply calls the `tf.matmul()` function, so the following lines are\n",
      "    equivalent:\n",
      "    \n",
      "    >>> d = a @ b @ [[10], [11]]\n",
      "    >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
      "    \n",
      "    Args:\n",
      "      a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
      "        `complex64`, `complex128` and rank > 1.\n",
      "      b: `tf.Tensor` with same type and rank as `a`.\n",
      "      transpose_a: If `True`, `a` is transposed before multiplication.\n",
      "      transpose_b: If `True`, `b` is transposed before multiplication.\n",
      "      adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      "        multiplication.\n",
      "      adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      "        multiplication.\n",
      "      a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
      "        **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      "        that assume most values in `a` are zero.\n",
      "        See `tf.sparse.sparse_dense_matmul`\n",
      "        for some support for `tf.sparse.SparseTensor` multiplication.\n",
      "      b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
      "        **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
      "        that assume most values in `a` are zero.\n",
      "        See `tf.sparse.sparse_dense_matmul`\n",
      "        for some support for `tf.sparse.SparseTensor` multiplication.\n",
      "      name: Name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
      "      is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      "      transpose or adjoint attributes are `False`:\n",
      "    \n",
      "      `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
      "      for all indices `i`, `j`.\n",
      "    \n",
      "      Note: This is matrix product, not element-wise product.\n",
      "    \n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
      "        `adjoint_b` are both set to `True`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.matmul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recopilación estadística\n",
    "\n",
    "* tf.reduce_min/max/mean: \n",
    "* tf.argmax()/tf.argmin() \n",
    "* tf.equal()\n",
    "* tf.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function argmax_v2 in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "argmax_v2(input, axis=None, output_type=tf.int64, name=None)\n",
      "    Returns the index with the largest value across axes of a tensor.\n",
      "    \n",
      "    In case of identity returns the smallest index.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> A = tf.constant([2, 20, 30, 3, 6])\n",
      "    >>> tf.math.argmax(A)  # A[2] is maximum in tensor A\n",
      "    <tf.Tensor: shape=(), dtype=int64, numpy=2>\n",
      "    >>> B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],\n",
      "    ...                  [14, 45, 23, 5, 27]])\n",
      "    >>> tf.math.argmax(B, 0)\n",
      "    <tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>\n",
      "    >>> tf.math.argmax(B, 1)\n",
      "    <tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>\n",
      "    >>> C = tf.constant([0, 0, 0, 0])\n",
      "    >>> tf.math.argmax(C) # Returns smallest index in case of ties\n",
      "    <tf.Tensor: shape=(), dtype=int64, numpy=0>\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`.\n",
      "      axis: An integer, the axis to reduce across. Default to 0.\n",
      "      output_type: An optional output dtype (`tf.int32` or `tf.int64`). Defaults\n",
      "        to `tf.int64`.\n",
      "      name: An optional name for the operation.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` of type `output_type`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  2  3]\n",
      " [ 2  5  8]\n",
      " [ 7 10  9]]\n"
     ]
    }
   ],
   "source": [
    "argmax_sample_1 = tf.constant([[7,2,3],[2,5,8],[7,10,9]])\n",
    "print(argmax_sample_1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones aritméticas basadas en dimensiones\n",
    "\n",
    "* tf.reduce_sum\n",
    "* tf.reduce_prod\n",
    "* tf.reduce_min \n",
    "* tf.reduce_max\n",
    "* tf.reduce_mean\n",
    "* tf.reduce_all (Y lógico)\n",
    "* tf.reduce_any (O lógico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_sum in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_sum(input_tensor, axis=None, keepdims=False, name=None)\n",
      "    Computes the sum of elements across dimensions of a tensor.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `axis`.\n",
      "    Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    entry in `axis`. If `keepdims` is true, the reduced dimensions\n",
      "    are retained with length 1.\n",
      "    \n",
      "    If `axis` is None, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> # x has a shape of (2, 3) (two rows and three columns):\n",
      "    >>> x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
      "    >>> x.numpy()\n",
      "    array([[1, 1, 1],\n",
      "           [1, 1, 1]], dtype=int32)\n",
      "    >>> # sum all the elements\n",
      "    >>> # 1 + 1 + 1 + 1 + 1+ 1 = 6\n",
      "    >>> tf.reduce_sum(x).numpy()\n",
      "    6\n",
      "    >>> # reduce along the first dimension\n",
      "    >>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "    >>> tf.reduce_sum(x, 0).numpy()\n",
      "    array([2, 2, 2], dtype=int32)\n",
      "    >>> # reduce along the second dimension\n",
      "    >>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]\n",
      "    >>> tf.reduce_sum(x, 1).numpy()\n",
      "    array([3, 3], dtype=int32)\n",
      "    >>> # keep the original dimensions\n",
      "    >>> tf.reduce_sum(x, 1, keepdims=True).numpy()\n",
      "    array([[3],\n",
      "           [3]], dtype=int32)\n",
      "    >>> # reduce along both dimensions\n",
      "    >>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6\n",
      "    >>> # or, equivalently, reduce along rows, then reduce the resultant array\n",
      "    >>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n",
      "    >>> # 2 + 2 + 2 = 6\n",
      "    >>> tf.reduce_sum(x, [0, 1]).numpy()\n",
      "    6\n",
      "    \n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      axis: The dimensions to reduce. If `None` (the default), reduces all\n",
      "        dimensions. Must be in the range `[-rank(input_tensor),\n",
      "        rank(input_tensor)]`.\n",
      "      keepdims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor, of the same dtype as the input_tensor.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to\n",
      "    int64 while tensorflow returns the same dtype as the input.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reduce_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "reduce_sample_1 = tf.constant([1,2,3,4,5,6],shape=[2,3])\n",
    "print(reduce_sample_1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando la suma a TODOS los elementos (axis=None) 21\n"
     ]
    }
   ],
   "source": [
    "print('Calculando la suma a TODOS los elementos (axis=None)',tf.reduce_sum(reduce_sample_1,axis=None).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando por columna (axis=0) [5 7 9]\n"
     ]
    }
   ],
   "source": [
    "print('Calculando por columna (axis=0)',tf.reduce_sum(reduce_sample_1,axis=0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando por fila (axis=1) [ 6 15]\n"
     ]
    }
   ],
   "source": [
    "print('Calculando por fila (axis=1)',tf.reduce_sum(reduce_sample_1,axis=1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contanenación - División"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenación:\n",
    "* tf.concat \n",
    "* tf.stack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "concat(values, axis, name='concat')\n",
      "    Concatenates tensors along one dimension.\n",
      "    \n",
      "    See also `tf.tile`, `tf.stack`, `tf.repeat`.\n",
      "    \n",
      "    Concatenates the list of tensors `values` along dimension `axis`.  If\n",
      "    `values[i].shape = [D0, D1, ... Daxis(i), ...Dn]`, the concatenated\n",
      "    result has shape\n",
      "    \n",
      "        [D0, D1, ... Raxis, ...Dn]\n",
      "    \n",
      "    where\n",
      "    \n",
      "        Raxis = sum(Daxis(i))\n",
      "    \n",
      "    That is, the data from the input tensors is joined along the `axis`\n",
      "    dimension.\n",
      "    \n",
      "    The number of dimensions of the input tensors must match, and all dimensions\n",
      "    except `axis` must be equal.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> t1 = [[1, 2, 3], [4, 5, 6]]\n",
      "    >>> t2 = [[7, 8, 9], [10, 11, 12]]\n",
      "    >>> tf.concat([t1, t2], 0)\n",
      "    <tf.Tensor: shape=(4, 3), dtype=int32, numpy=\n",
      "    array([[ 1,  2,  3],\n",
      "           [ 4,  5,  6],\n",
      "           [ 7,  8,  9],\n",
      "           [10, 11, 12]], dtype=int32)>\n",
      "    \n",
      "    >>> tf.concat([t1, t2], 1)\n",
      "    <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
      "    array([[ 1,  2,  3,  7,  8,  9],\n",
      "           [ 4,  5,  6, 10, 11, 12]], dtype=int32)>\n",
      "    \n",
      "    As in Python, the `axis` could also be negative numbers. Negative `axis`\n",
      "    are interpreted as counting from the end of the rank, i.e.,\n",
      "     `axis + rank(values)`-th dimension.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]\n",
      "    >>> t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]\n",
      "    >>> tf.concat([t1, t2], -1)\n",
      "    <tf.Tensor: shape=(2, 2, 4), dtype=int32, numpy=\n",
      "      array([[[ 1,  2,  7,  4],\n",
      "              [ 2,  3,  8,  4]],\n",
      "             [[ 4,  4,  2, 10],\n",
      "              [ 5,  3, 15, 11]]], dtype=int32)>\n",
      "    \n",
      "    Note: If you are concatenating along a new axis consider using stack.\n",
      "    E.g.\n",
      "    \n",
      "    ```python\n",
      "    tf.concat([tf.expand_dims(t, axis) for t in tensors], axis)\n",
      "    ```\n",
      "    \n",
      "    can be rewritten as\n",
      "    \n",
      "    ```python\n",
      "    tf.stack(tensors, axis=axis)\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      values: A list of `Tensor` objects or a single `Tensor`.\n",
      "      axis: 0-D `int32` `Tensor`.  Dimension along which to concatenate. Must be\n",
      "        in the range `[-rank(values), rank(values))`. As in Python, indexing for\n",
      "        axis is 0-based. Positive axis in the rage of `[0, rank(values))` refers\n",
      "        to `axis`-th dimension. And negative axis refers to `axis +\n",
      "        rank(values)`-th dimension.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` resulting from concatenation of the input tensors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim originales (4, 100, 100, 3) (4, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "concat_sample_1 = tf.random.normal([4,100,100,3])\n",
    "concat_sample_2 = tf.random.normal([4,100,100,3])\n",
    "\n",
    "print('Dim originales',concat_sample_1.shape,concat_sample_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim del tensor concatenado (4, 100, 100, 6)\n"
     ]
    }
   ],
   "source": [
    "concated_sample_1 = tf.concat([concat_sample_1,concat_sample_2],axis=3)\n",
    "print('Dim del tensor concatenado',concated_sample_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stack in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "stack(values, axis=0, name='stack')\n",
      "    Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor.\n",
      "    \n",
      "    See also `tf.concat`, `tf.tile`, `tf.repeat`.\n",
      "    \n",
      "    Packs the list of tensors in `values` into a tensor with rank one higher than\n",
      "    each tensor in `values`, by packing them along the `axis` dimension.\n",
      "    Given a list of length `N` of tensors of shape `(A, B, C)`;\n",
      "    \n",
      "    if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\n",
      "    if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\n",
      "    Etc.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> x = tf.constant([1, 4])\n",
      "    >>> y = tf.constant([2, 5])\n",
      "    >>> z = tf.constant([3, 6])\n",
      "    >>> tf.stack([x, y, z])\n",
      "    <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "    array([[1, 4],\n",
      "           [2, 5],\n",
      "           [3, 6]], dtype=int32)>\n",
      "    >>> tf.stack([x, y, z], axis=1)\n",
      "    <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      "    array([[1, 2, 3],\n",
      "           [4, 5, 6]], dtype=int32)>\n",
      "    \n",
      "    This is the opposite of unstack.  The numpy equivalent is `np.stack`\n",
      "    \n",
      "    >>> np.array_equal(np.stack([x, y, z]), tf.stack([x, y, z]))\n",
      "    True\n",
      "    \n",
      "    Args:\n",
      "      values: A list of `Tensor` objects with the same shape and type.\n",
      "      axis: An `int`. The axis to stack along. Defaults to the first dimension.\n",
      "        Negative values wrap around, so the valid range is `[-(R+1), R+1)`.\n",
      "      name: A name for this operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      output: A stacked `Tensor` with the same type as `values`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `axis` is out of the range [-(R+1), R+1).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim originales (100, 100, 3) (100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "stack_sample_1 = tf.random.normal([100,100,3])\n",
    "stack_sample_2 = tf.random.normal([100,100,3])\n",
    "\n",
    "print('Dim originales',stack_sample_1.shape,stack_sample_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimn nueva (2, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "stacked_sample_1 = tf.stack([stack_sample_1,stack_sample_2])\n",
    "print('Dimn nueva',stacked_sample_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de tensores \n",
    "\n",
    "* tf.unstack() - Divide un tensor por una dim especifica\n",
    "* tf.split() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function unstack in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "unstack(value, num=None, axis=0, name='unstack')\n",
      "    Unpacks the given dimension of a rank-`R` tensor into rank-`(R-1)` tensors.\n",
      "    \n",
      "    Unpacks `num` tensors from `value` by chipping it along the `axis` dimension.\n",
      "    If `num` is not specified (the default), it is inferred from `value`'s shape.\n",
      "    If `value.shape[axis]` is not known, `ValueError` is raised.\n",
      "    \n",
      "    For example, given a tensor of shape `(A, B, C, D)`;\n",
      "    \n",
      "    If `axis == 0` then the i'th tensor in `output` is the slice\n",
      "      `value[i, :, :, :]` and each tensor in `output` will have shape `(B, C, D)`.\n",
      "      (Note that the dimension unpacked along is gone, unlike `split`).\n",
      "    \n",
      "    If `axis == 1` then the i'th tensor in `output` is the slice\n",
      "      `value[:, i, :, :]` and each tensor in `output` will have shape `(A, C, D)`.\n",
      "    Etc.\n",
      "    \n",
      "    This is the opposite of stack.\n",
      "    \n",
      "    Args:\n",
      "      value: A rank `R > 0` `Tensor` to be unstacked.\n",
      "      num: An `int`. The length of the dimension `axis`. Automatically inferred if\n",
      "        `None` (the default).\n",
      "      axis: An `int`. The axis to unstack along. Defaults to the first dimension.\n",
      "        Negative values wrap around, so the valid range is `[-R, R)`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The list of `Tensor` objects unstacked from `value`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `num` is unspecified and cannot be inferred.\n",
      "      ValueError: If `axis` is out of the range [-R, R).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.unstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 100, 100, 3])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_sample_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(100, 100, 3), dtype=float32, numpy=\n",
       " array([[[-1.62930727e+00, -1.91322780e+00, -2.73170531e-01],\n",
       "         [ 1.09348404e+00,  4.64917213e-01,  8.57612610e-01],\n",
       "         [ 1.13103867e+00,  7.25233376e-01,  1.01267493e+00],\n",
       "         ...,\n",
       "         [-2.00934625e+00,  7.82824099e-01,  2.82770824e-02],\n",
       "         [ 5.13250411e-01,  9.47809160e-01,  2.11677905e-02],\n",
       "         [ 5.31098068e-01,  2.21277744e-01,  1.83563590e-01]],\n",
       " \n",
       "        [[ 7.78314531e-01, -1.64613080e+00, -5.36854267e-01],\n",
       "         [-5.97901531e-02, -4.14725721e-01,  5.19917250e-01],\n",
       "         [ 1.03787100e+00,  8.33033979e-01,  2.33838946e-01],\n",
       "         ...,\n",
       "         [ 6.48794353e-01, -1.70689070e+00,  9.60010827e-01],\n",
       "         [-3.29141617e-01,  4.99220282e-01, -7.53772676e-01],\n",
       "         [-7.54969537e-01,  8.64554644e-01,  8.69382083e-01]],\n",
       " \n",
       "        [[ 1.18388367e+00, -9.84409451e-01,  6.05888784e-01],\n",
       "         [ 1.02480459e+00, -9.02650692e-03,  1.58609793e-01],\n",
       "         [ 7.81348348e-01,  7.35665739e-01,  2.66455472e-01],\n",
       "         ...,\n",
       "         [ 5.31358600e-01,  1.03425789e+00, -6.68036789e-02],\n",
       "         [-4.91088539e-01, -5.77592373e-01, -2.52116054e-01],\n",
       "         [-1.26461637e+00, -2.64471948e-01,  6.03108644e-01]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 7.93019831e-01, -1.26157188e+00, -1.88505322e-01],\n",
       "         [ 7.02512443e-01,  1.18683139e-03, -7.64701724e-01],\n",
       "         [-1.31836498e+00,  1.09540546e+00,  8.58311236e-01],\n",
       "         ...,\n",
       "         [ 8.35545242e-01, -2.55398422e-01,  1.75313115e+00],\n",
       "         [ 4.20682877e-01,  7.66856134e-01, -1.47005415e+00],\n",
       "         [-6.65819526e-01, -1.07218015e+00,  4.32355785e+00]],\n",
       " \n",
       "        [[-7.89969087e-01, -1.20444489e+00, -1.14137423e+00],\n",
       "         [ 8.60744655e-01, -7.17745781e-01, -8.20264459e-01],\n",
       "         [ 3.54736090e-01, -1.94494396e-01, -1.16924606e-01],\n",
       "         ...,\n",
       "         [ 6.01288915e-01, -2.18951300e-01,  1.70594081e-01],\n",
       "         [-7.32546568e-01,  1.57534516e+00, -2.72576272e-01],\n",
       "         [ 6.38635457e-01, -1.51897180e+00, -1.17549360e+00]],\n",
       " \n",
       "        [[ 6.69462562e-01,  1.74466765e+00,  1.72061849e+00],\n",
       "         [ 6.48112416e-01,  3.51755589e-01,  8.54532540e-01],\n",
       "         [ 2.58567721e-01, -2.35680056e+00, -4.16806281e-01],\n",
       "         ...,\n",
       "         [ 1.41734910e+00,  5.94567120e-01, -1.05324650e+00],\n",
       "         [-9.02567580e-02, -1.88994467e-01,  1.41092986e-01],\n",
       "         [-2.54257262e-01, -1.15734863e+00, -7.67784357e-01]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(100, 100, 3), dtype=float32, numpy=\n",
       " array([[[ 1.0820777 ,  0.26284888, -0.99874276],\n",
       "         [-0.9170239 ,  0.42767212, -1.0269595 ],\n",
       "         [-0.4087604 , -1.1190727 , -1.0407794 ],\n",
       "         ...,\n",
       "         [ 1.9793031 ,  0.91025645,  1.2682252 ],\n",
       "         [-0.34876925,  1.1741776 , -0.8954506 ],\n",
       "         [ 0.1258188 , -0.99866295, -1.0165043 ]],\n",
       " \n",
       "        [[ 0.22045763,  0.7641804 , -1.8102411 ],\n",
       "         [ 2.6414886 , -1.5460045 , -0.8755924 ],\n",
       "         [-0.36339542, -1.4568416 , -0.34268054],\n",
       "         ...,\n",
       "         [ 0.12991197,  0.78993464, -0.33318466],\n",
       "         [-0.2541255 , -1.3707007 ,  0.48722458],\n",
       "         [ 0.3866653 , -1.6464767 ,  0.6663657 ]],\n",
       " \n",
       "        [[ 0.45800194, -1.290625  , -1.1042808 ],\n",
       "         [-0.42633966,  1.7724818 ,  0.847589  ],\n",
       "         [-0.1925935 , -1.3361357 , -1.3348594 ],\n",
       "         ...,\n",
       "         [ 0.67617047,  0.7750943 , -0.03512094],\n",
       "         [ 0.6925163 ,  0.10349545, -0.78207207],\n",
       "         [ 0.32667553, -0.15929909, -0.18922774]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-2.1015918 , -0.41677237, -1.2856964 ],\n",
       "         [ 1.2451975 ,  2.0125945 ,  1.0553635 ],\n",
       "         [-0.12755345,  0.19915052,  2.74433   ],\n",
       "         ...,\n",
       "         [ 0.33007962,  0.111542  , -0.64213824],\n",
       "         [-0.6161552 ,  2.1504707 ,  0.18471165],\n",
       "         [-0.7516763 ,  0.87169915, -0.680539  ]],\n",
       " \n",
       "        [[-0.573405  ,  2.2742293 ,  0.4501755 ],\n",
       "         [-0.1440016 , -1.152886  , -1.0243567 ],\n",
       "         [-0.16458909, -1.3690369 ,  0.321977  ],\n",
       "         ...,\n",
       "         [ 0.8788206 , -0.23126008, -0.55356026],\n",
       "         [-0.9453941 ,  0.1126816 , -0.9413515 ],\n",
       "         [ 1.0749685 , -0.14397292,  0.31306317]],\n",
       " \n",
       "        [[-0.7609668 ,  0.43579835,  0.77978474],\n",
       "         [-1.5267471 ,  0.5945059 , -0.19959834],\n",
       "         [ 0.5083115 ,  0.29004824,  0.3947293 ],\n",
       "         ...,\n",
       "         [-0.35609236, -1.1875703 , -0.54299694],\n",
       "         [-0.47744995, -1.9095455 ,  0.91606605],\n",
       "         [-0.8041896 ,  1.7274415 ,  0.7416168 ]]], dtype=float32)>]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(stacked_sample_1,axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function split in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "split(value, num_or_size_splits, axis=0, num=None, name='split')\n",
      "    Splits a tensor `value` into a list of sub tensors.\n",
      "    \n",
      "    See also `tf.unstack`.\n",
      "    \n",
      "    If `num_or_size_splits` is an integer,  then `value` is split along the\n",
      "    dimension `axis` into `num_or_size_splits` smaller tensors. This requires that\n",
      "    `value.shape[axis]` is divisible by `num_or_size_splits`.\n",
      "    \n",
      "    If `num_or_size_splits` is a 1-D Tensor (or list), then `value` is split into\n",
      "    `len(num_or_size_splits)` elements. The shape of the `i`-th\n",
      "    element has the same size as the `value` except along dimension `axis` where\n",
      "    the size is `num_or_size_splits[i]`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    >>> x = tf.Variable(tf.random.uniform([5, 30], -1, 1))\n",
      "    >>>\n",
      "    >>> # Split `x` into 3 tensors along dimension 1\n",
      "    >>> s0, s1, s2 = tf.split(x, num_or_size_splits=3, axis=1)\n",
      "    >>> tf.shape(s0).numpy()\n",
      "    array([ 5, 10], dtype=int32)\n",
      "    >>>\n",
      "    >>> # Split `x` into 3 tensors with sizes [4, 15, 11] along dimension 1\n",
      "    >>> split0, split1, split2 = tf.split(x, [4, 15, 11], 1)\n",
      "    >>> tf.shape(split0).numpy()\n",
      "    array([5, 4], dtype=int32)\n",
      "    >>> tf.shape(split1).numpy()\n",
      "    array([ 5, 15], dtype=int32)\n",
      "    >>> tf.shape(split2).numpy()\n",
      "    array([ 5, 11], dtype=int32)\n",
      "    \n",
      "    Args:\n",
      "      value: The `Tensor` to split.\n",
      "      num_or_size_splits: Either an integer indicating the number of splits along\n",
      "        `axis` or a 1-D integer `Tensor` or Python list containing the sizes of\n",
      "        each output tensor along `axis`. If a scalar, then it must evenly divide\n",
      "        `value.shape[axis]`; otherwise the sum of sizes along the split axis\n",
      "        must match that of the `value`.\n",
      "      axis: An integer or scalar `int32` `Tensor`. The dimension along which to\n",
      "        split. Must be in the range `[-rank(value), rank(value))`. Defaults to 0.\n",
      "      num: Optional, used to specify the number of outputs when it cannot be\n",
      "        inferred from the shape of `size_splits`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      if `num_or_size_splits` is a scalar returns a list of `num_or_size_splits`\n",
      "      `Tensor` objects; if `num_or_size_splits` is a 1-D Tensor returns\n",
      "      `num_or_size_splits.get_shape[0]` `Tensor` objects resulting from splitting\n",
      "      `value`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `num` is unspecified and cannot be inferred.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre el parametro num_or_size_splits\n",
    "\n",
    "* entero, el tensor se divide UNIFORMEMENTE en subtensores en la dimensión que se le espeficique\n",
    "* vector, el tensor se divide en subtensores segun el valor del elemento del VECTOR, en la dimensión especificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims originales (10, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "split_sample_1 = tf.random.normal([10,100,100,3])\n",
    "print('Dims originales', split_sample_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "splited_sample_1 = tf.split(split_sample_1, num_or_size_splits=5)\n",
    "print(np.shape(splited_sample_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 100, 100, 3)\n",
      "(5, 100, 100, 3)\n",
      "(2, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "splited_sample_2 = tf.split(split_sample_1, num_or_size_splits=[3,5,2])\n",
    "\n",
    "print(np.shape(splited_sample_2[0]))\n",
    "print(np.shape(splited_sample_2[1]))\n",
    "print(np.shape(splited_sample_2[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting de tensores\n",
    "\n",
    "* tf.sort()\n",
    "* tf.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sort in module tensorflow.python.ops.sort_ops:\n",
      "\n",
      "sort(values, axis=-1, direction='ASCENDING', name=None)\n",
      "    Sorts a tensor.\n",
      "    \n",
      "    Usage:\n",
      "    \n",
      "    ```python\n",
      "    import tensorflow as tf\n",
      "    a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n",
      "    b = tf.sort(a,axis=-1,direction='ASCENDING',name=None)\n",
      "    c = tf.keras.backend.eval(b)\n",
      "    # Here, c = [  1.     2.8   10.    26.9   62.3  166.32]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      values: 1-D or higher numeric `Tensor`.\n",
      "      axis: The axis along which to sort. The default is -1, which sorts the last\n",
      "        axis.\n",
      "      direction: The direction in which to sort the values (`'ASCENDING'` or\n",
      "        `'DESCENDING'`).\n",
      "      name: Optional name for the operation.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` with the same dtype and shape as `values`, with the elements\n",
      "          sorted along the given `axis`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If axis is not a constant scalar, or the direction is invalid.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 7 6 5 0 4 9 2 3 1]\n"
     ]
    }
   ],
   "source": [
    "sort_sample_1 = tf.random.shuffle(tf.range(10))\n",
    "print(sort_sample_1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 8 7 6 5 4 3 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "sorted_sample_1 = tf.sort(sort_sample_1,direction='DESCENDING')\n",
    "print(sorted_sample_1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
